{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "834c9266-dd67-464d-b30d-eb56aee77724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam\n",
    "import os\n",
    "import sys\n",
    "from sys import getsizeof\n",
    "import time\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "directory_path = os.path.abspath(os.path.join('../src/'))\n",
    "if directory_path not in sys.path:\n",
    "    sys.path.append(directory_path)\n",
    "    \n",
    "from read_process import get_contig_lengths_dict,\\\n",
    "incorporate_replaced_pos_info,incorporate_insertions_and_deletions,\\\n",
    "get_positions_from_md_tag,reverse_complement,get_edit_information,get_edit_information_wrapper,\\\n",
    "has_edits,get_total_coverage_for_contig_at_position,\\\n",
    "print_read_info, update_coverage_array, get_read_information, get_hamming_distance, remove_softclipped_bases,find\n",
    "\n",
    "from utils import get_intervals, index_bam, write_rows_to_info_file, write_header_to_bam, \\\n",
    "write_read_to_bam_file, remove_file_if_exists, make_folder\n",
    "\n",
    "import os, psutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a51607-e768-4314-939b-d59068863e5d",
   "metadata": {},
   "source": [
    "# Preload which barcodes to use..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a707e5c-8b77-4952-91af-c3f90802dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "barcodes_list_path = '/projects/ps-yeolab3/ekofman/Sammi/MouseBrainEF1A_SingleCell_EPR_batch2/cellranger/results/ms_hippo_stamp_EIF4A_batch2/outs/filtered_feature_bc_matrix/barcodes.tsv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09215472-0794-47c5-a306-5461de94d169",
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode_whitelist = set(pd.read_csv(barcodes_list_path, names=['barcodes']).barcodes.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b41cb49-f4b4-464f-ae2b-19d808f210e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(barcode_whitelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72010a82-87a7-4198-aa66-79f57ad70337",
   "metadata": {},
   "source": [
    "# ~~~~~~~~~~~~~~~~~~\n",
    "# Multi-processing enabled\n",
    "# ~~~~~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7166d4-3f54-4108-abb8-e978f704727a",
   "metadata": {},
   "source": [
    "# An example on a full 10x bam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5962296-f5be-4c0f-8323-74f591d58dea",
   "metadata": {},
   "source": [
    "#### in 10X's bam file, xf=25 means that read is uniquely mapped to a genome, and was used for counting UMI. So we should only look at reads with xf=25 from the 10X bam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fba3a6f1-60ab-4ec3-9f17-a59ab8d4bbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bampath = '/projects/ps-yeolab5/ekofman/Sammi/MouseBrainEF1A_SingleCell_EPR_batch2/filtered_possorted_ms_hippo_stamp_bam/filtered_keep_xf25_possorted_genome_with_header.bam_MD.bam'\n",
    "bampath = '/projects/ps-yeolab3/ekofman/sailor2/data/groups_0_1_2_3_4_5_6_7_8_9_10_11_merged.bam'\n",
    "\n",
    "\n",
    "samfile = pysam.AlignmentFile(bampath, \"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "606494dd-4fd1-4760-9ad7-2c201a19e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "samfile_header = str(samfile.header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cec4616a-69eb-477c-a072-b15d570369d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19338.323"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getsizeof(samfile_header)/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ea541e-ab33-40a6-9d12-88c707c3932c",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "842901f9-7341-4c1f-87c7-0dd89cd8ac8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_edits(bampath, contig, split_index, start, end, output_folder, barcode_whitelist=None, verbose=False):  \n",
    "    time_reporting = {}\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    samfile = pysam.AlignmentFile(bampath, \"rb\")\n",
    "        \n",
    "    counts = defaultdict(lambda:defaultdict(lambda:0))\n",
    "    total_reads = 0\n",
    "    \n",
    "    bam_handles_for_barcodes = {}\n",
    "    read_lists_for_barcodes = defaultdict(lambda:[])\n",
    "    \n",
    "    reads_for_contig = samfile.fetch(contig, start, end, multiple_iterators=True)\n",
    "\n",
    "    output_file = '{}/{}_{}_{}_{}_edit_info.tsv'.format(edit_info_subfolder, contig, split_index, start, end)\n",
    "    remove_file_if_exists(output_file)\n",
    "\n",
    "    with open(output_file, 'w') as f:        \n",
    "        write_header_to_bam(f)\n",
    "\n",
    "        for i, read in enumerate(reads_for_contig):\n",
    "            total_reads += 1\n",
    "            \n",
    "            if total_reads % 1000 == 0:\n",
    "                time_reporting[total_reads] = time.perf_counter() - start_time\n",
    "\n",
    "            barcode = read.get_tag(\"CB\")\n",
    "            if barcode_whitelist:\n",
    "                if barcode not in barcode_whitelist:\n",
    "                    counts[contig]['Barcode Filtered'] += 1\n",
    "                    continue\n",
    "                \n",
    "            barcodes[contig][barcode] += 1\n",
    "\n",
    "            verbose = False\n",
    "            \n",
    "            try:\n",
    "                error_code, list_of_rows, num_edits_of_each_type = get_read_information(read, contig, verbose=verbose)\n",
    "            except Exception as e:\n",
    "                print(\"Failed on\\n{}\".format(read.to_string()))\n",
    "                break\n",
    "                \n",
    "            if error_code:\n",
    "                counts[contig][error_code] += 1\n",
    "            else:\n",
    "                counts[contig][EDITED_CODE] += 1\n",
    "                write_rows_to_info_file(list_of_rows, f)\n",
    "            \n",
    "            # Store each read using its string representation\n",
    "            read_as_string = read.to_string()\n",
    "            read_tab_separated = read_as_string.split('\\t')\n",
    "     \n",
    "            second_new_contig_section = '{}_{}'.format(contig, barcode)\n",
    "            read_tab_separated[2] = second_new_contig_section\n",
    "            \n",
    "            read_as_string = '\\t'.join(read_tab_separated)\n",
    "            \n",
    "            read_lists_for_barcodes[barcode].append(read_as_string)\n",
    "            \n",
    "    \n",
    "    # Add all reads to dictionary for contig and barcode, in their string representation\n",
    "    num_barcodes = 0\n",
    "    total_bams = len(read_lists_for_barcodes)\n",
    "    \n",
    "    \n",
    "    barcode_to_concatted_reads = {}\n",
    "    for barcode, read_list in read_lists_for_barcodes.items():        \n",
    "        num_barcodes += 1\n",
    "        if num_barcodes % 100 == 0:\n",
    "            #print('{}/{} processed'.format(num_barcodes, total_bams))\n",
    "            pass\n",
    "        # Concatenate the string representations of all reads for each bam-contig combination\n",
    "        all_reads_concatted = '\\n'.join(read_list)\n",
    "            \n",
    "        # Save this concatenated block of text to dictionary\n",
    "        barcode_to_concatted_reads[barcode] = all_reads_concatted\n",
    "        \n",
    "    time_reporting[total_reads] = time.perf_counter() - start_time\n",
    "    \n",
    "    samfile.close()\n",
    "    \n",
    "    return barcode_to_concatted_reads, total_reads, barcodes, counts, time_reporting\n",
    "\n",
    "\n",
    "def find_edits_and_split_bams(bampath, contig, split_index, start, end, output_folder, barcode_whitelist=None, verbose=False):\n",
    "    barcode_to_concatted_reads, total_reads, barcodes, counts, time_reporting = find_edits(bampath, contig, split_index,\n",
    "                                                                         start, end, output_folder, barcode_whitelist=barcode_whitelist, verbose=verbose)    \n",
    "    return barcode_to_concatted_reads, total_reads, barcodes, counts, time_reporting\n",
    "    \n",
    "def find_edits_and_split_bams_wrapper(parameters):\n",
    "    try:\n",
    "        start_time = time.perf_counter()\n",
    "        bampath, contig, split_index, start, end, output_folder, barcode_whitelist, verbose = parameters\n",
    "        label = '{}({}):{}-{}'.format(contig, split_index, start, end)\n",
    "\n",
    "        #print(\"{} ({}):{}-{}\\tfind_edits_and_split_bams\".format(contig, split_index, start, end))\n",
    "        barcode_to_concatted_reads, total_reads, barcodes, counts, time_reporting = find_edits_and_split_bams(bampath, contig, split_index, start, end,                                                                                        \n",
    "                                                                                                              output_folder, \n",
    "                                                                                                              barcode_whitelist=barcode_whitelist,\n",
    "                                                                                                              verbose=False)\n",
    "        barcodes_df = pd.DataFrame.from_dict(barcodes)\n",
    "        counts_df = pd.DataFrame.from_dict(counts)\n",
    "        time_df = pd.DataFrame.from_dict(time_reporting, orient='index')\n",
    "        if len(barcode_to_concatted_reads) > 0:\n",
    "            barcode_to_concatted_reads_pl = pl.from_dict(barcode_to_concatted_reads).transpose(include_header=True, header_name='barcode').rename({\"column_0\": \"contents\"})\n",
    "        else:\n",
    "            # No transposes are allowed on empty dataframes\n",
    "            barcode_to_concatted_reads_pl = pl.from_dict(barcode_to_concatted_reads)\n",
    "            \n",
    "        total_time = time.perf_counter() - start_time\n",
    "        return contig, label, barcode_to_concatted_reads_pl, total_reads, barcodes_df, counts_df, time_df, total_time\n",
    "    except Exception as e:\n",
    "        print('Contig {}: {}'.format(label, e))\n",
    "        return 0, pd.DataFrame(), label, pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e80d3e0-398f-49e2-bf55-281a9621698f",
   "metadata": {},
   "source": [
    "# Go through every read and identify all edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5481bde1-a0e7-43b6-8699-1043a6d7cae3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU count: 64\n",
      "Contig 1\n",
      "Contig 10\n",
      "Contig 11\n",
      "Contig 12\n",
      "Contig 13\n",
      "Contig 14\n",
      "Contig 15\n",
      "Contig 16\n",
      "Contig 17\n",
      "Contig 18\n",
      "Contig 19\n",
      "Contig 2\n",
      "Contig 3\n",
      "Contig 4\n",
      "Contig 5\n",
      "Contig 6\n",
      "Contig 7\n",
      "Contig 8\n",
      "Contig 9\n",
      "Contig MT\n",
      "Contig X\n",
      "Contig Y\n",
      "352 total jobs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 352/352 [00:44<00:00,  7.86it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "#from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "print(\"CPU count: {}\".format(multiprocessing.cpu_count()))\n",
    "\n",
    "output_folder = '/projects/ps-yeolab3/ekofman/sailor2/scripts/full_test-highmem_bccontig'\n",
    "\n",
    "contig_lengths_dict = get_contig_lengths_dict(samfile)\n",
    "\n",
    "# Print info?\n",
    "verbose = False \n",
    "EDITED_CODE = 'edited'\n",
    "\n",
    "# How many subcontigs to split each contig into to leverage multi-processing\n",
    "num_intervals = 16\n",
    "\n",
    "num_reads_to_coverage_dict_kb = {}\n",
    "num_reads_to_seconds = {}\n",
    "\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "total_seconds_for_reads = {0: 1}\n",
    "\n",
    "barcodes = defaultdict(lambda:defaultdict(lambda:0))\n",
    "\n",
    "jobs = []\n",
    "for contig in contig_lengths_dict.keys():\n",
    "    # Skip useless contigs\n",
    "    if len(contig) > 5 or contig == 'Stamp':# or contig != '17':\n",
    "        continue\n",
    "        \n",
    "    print(\"Contig {}\".format(contig))\n",
    "    contig_length = contig_lengths_dict.get(contig)\n",
    "    intervals_for_contig = get_intervals(contig, contig_lengths_dict, num_intervals)\n",
    "    \n",
    "    # Make subfolder in which to information about edits\n",
    "    edit_info_subfolder = '{}/edit_info'.format(output_folder)\n",
    "    make_folder(edit_info_subfolder)\n",
    "        \n",
    "    # Set up for pool\n",
    "    for split_index, interval in enumerate(intervals_for_contig):\n",
    "        split_index = str(split_index).zfill(3)\n",
    "        parameters = [bampath, contig, split_index, interval[0], interval[1], output_folder, barcode_whitelist, verbose]\n",
    "        jobs.append(parameters)\n",
    "    \n",
    "print(\"{} total jobs\".format(len(jobs)))\n",
    "\n",
    "# Pooling\n",
    "results = []\n",
    "overall_total_reads = 0\n",
    "\n",
    "overall_label_to_list_of_contents = defaultdict(lambda:{})\n",
    "\n",
    "with Pool(processes=16) as p:\n",
    "    max_ = len(jobs)\n",
    "    with tqdm(total=max_) as pbar:\n",
    "        for _ in p.imap_unordered(find_edits_and_split_bams_wrapper, jobs):\n",
    "            pbar.update()\n",
    "            \n",
    "            overall_label_to_list_of_contents[_[0]][_[1]] =  _[2]\n",
    "            results.append([_[3], _[4], _[5], _[6], _[7]])\n",
    "            \n",
    "            total_reads = _[3]\n",
    "            total_time = time.perf_counter() - start_time\n",
    "            \n",
    "            overall_total_reads += total_reads\n",
    "\n",
    "            total_seconds_for_reads[overall_total_reads] = total_time\n",
    "\n",
    "overall_time = time.perf_counter() - start_time \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df0f1977-a6e9-4653-97ef-fb2afa07ad19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 45.14981406927109 seconds\n",
      "Total time: 0.7524969011545182 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"Total time: {} seconds\".format(overall_time))\n",
    "print(\"Total time: {} minutes\".format(overall_time/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74036ccf-01c5-49d3-8bcd-6bbca94f8a49",
   "metadata": {},
   "source": [
    "Memory: 230 Gigabytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7ad726-4045-4229-ae90-369ae6327f06",
   "metadata": {},
   "source": [
    "# More helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75188cf5-e1f2-4390-ba25-541e0f7b2800",
   "metadata": {},
   "source": [
    "# Combine all of the reads (string representation) for each barcode\n",
    "## Groups the results from each sub-contig segment above, for example the reads from the first half of chr1 and those from the second half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de955557-09d6-4d68-b415-8e16802c64ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall contigs:\n",
      "\n",
      "\t dict_keys(['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '3', '4', '5', '6', '7', '8', '9', 'MT', 'X', 'Y'])\n",
      "\n",
      "Subcontig regions for an example contig (1):\n",
      "\n",
      "\t ['1(000):0-12216999', '1(001):12216999-24433998', '1(002):24433998-36650997', '1(003):36650997-48867996', '1(004):48867996-61084995', '1(005):61084995-73301994', '1(006):73301994-85518993', '1(007):85518993-97735992', '1(008):97735992-109952991', '1(009):109952991-122169990', '1(010):122169990-134386989', '1(011):134386989-146603988', '1(012):146603988-158820987', '1(013):158820987-171037986', '1(014):171037986-183254985', '1(015):183254985-195471984']\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall contigs:\\n\\n\\t\", overall_label_to_list_of_contents.keys())\n",
    "print(\"\\nSubcontig regions for an example contig (1):\\n\\n\\t\",sorted(overall_label_to_list_of_contents.get('1').keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ee7632-3054-470c-890b-134083803011",
   "metadata": {},
   "source": [
    "450 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8134fb0e-6dd2-4348-9848-ee1cf666cd3b",
   "metadata": {},
   "source": [
    "### Generate list of jobs to be multiprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59781d69-3789-4528-b509-9b529b5ad199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_params(contig, df_dict, header_string):\n",
    "    job_params = []\n",
    "    \n",
    "    # Sort the subcontig regions such that the reads are properly ordered \n",
    "    sorted_subcontig_names = sorted(df_dict.keys())\n",
    "    sorted_subcontig_dfs = []\n",
    "    for n in sorted_subcontig_names:\n",
    "        sorted_subcontig_dfs.append(df_dict.get(n))\n",
    "        \n",
    "    if len(sorted_subcontig_dfs) == 0:\n",
    "        print(\"Empty\")\n",
    "        return []\n",
    "    \n",
    "    print(\"\\t{}: num subcontigs to concat: {}\".format(contig, len(sorted_subcontig_dfs)))\n",
    "    # All of the reads for all of the barcodes are in this dataframe\n",
    "    print(\"\\t{}: concatting\".format(contig))\n",
    "    all_contents_df = pl.concat(sorted_subcontig_dfs)\n",
    "        \n",
    "    # Combine the reads (in string representation) for all rows corresponding to a barcode        \n",
    "    for n in [\"A\", \"C\", \"G\", \"T\"]:\n",
    "        suffix = \"{}-1\".format(n)\n",
    "        \n",
    "        all_contents_for_suffix = all_contents_df.filter(pl.col('barcode').str.ends_with(suffix))\n",
    "        \n",
    "        reads_deduped = list(OrderedDict.fromkeys(all_contents_for_suffix.transpose().with_columns(\n",
    "            pl.concat_str(\n",
    "                [pl.col(c) for c in all_contents_for_suffix.transpose().columns],\n",
    "                separator=\"\\n\"\n",
    "                 ).alias(\"combined_text\")\n",
    "        )[['combined_text']][1].item().split('\\n')))\n",
    "        \n",
    "        # Make a sub-subfolder to put the bams for this specific contig\n",
    "        contig_folder = '{}/{}_{}/'.format(split_bams_folder, contig, n)\n",
    "        if not os.path.exists(contig_folder):\n",
    "            os.mkdir(contig_folder)\n",
    "            \n",
    "            \n",
    "        bam_file_name = '{}/{}_{}.bam'.format(contig_folder, contig, n)\n",
    "        \n",
    "        # Add parameters to list of jobs\n",
    "        job_params.append([reads_deduped, bam_file_name, header_string])\n",
    "        \n",
    "    del all_contents_df\n",
    "    return job_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6589a91c-43b3-44eb-9a06-ae430dba3c3b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1: num subcontigs to concat: 16\n",
      "\t1: concatting\n",
      "\t2: num subcontigs to concat: 16\n",
      "\t2: concatting\n",
      "\t3: num subcontigs to concat: 16\n",
      "\t3: concatting\n",
      "\t4: num subcontigs to concat: 16\n",
      "\t4: concatting\n",
      "\t5: num subcontigs to concat: 16\n",
      "\t5: concatting\n",
      "\t6: num subcontigs to concat: 16\n",
      "\t6: concatting\n",
      "\t7: num subcontigs to concat: 16\n",
      "\t7: concatting\n",
      "\t8: num subcontigs to concat: 16\n",
      "\t8: concatting\n",
      "\t9: num subcontigs to concat: 16\n",
      "\t9: concatting\n",
      "\t10: num subcontigs to concat: 16\n",
      "\t10: concatting\n",
      "\t11: num subcontigs to concat: 16\n",
      "\t11: concatting\n",
      "\t12: num subcontigs to concat: 16\n",
      "\t12: concatting\n",
      "\t13: num subcontigs to concat: 16\n",
      "\t13: concatting\n",
      "\t14: num subcontigs to concat: 16\n",
      "\t14: concatting\n",
      "\t15: num subcontigs to concat: 16\n",
      "\t15: concatting\n",
      "\t16: num subcontigs to concat: 16\n",
      "\t16: concatting\n",
      "\t17: num subcontigs to concat: 16\n",
      "\t17: concatting\n",
      "\t18: num subcontigs to concat: 16\n",
      "\t18: concatting\n",
      "\t19: num subcontigs to concat: 16\n",
      "\t19: concatting\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "# Get the bam header, which will be used for each of the split bams too\n",
    "header_string = str(samfile.header)\n",
    "\n",
    "# Make a subfolder into which the split bams will be placed\n",
    "split_bams_folder = '{}/split_bams'.format(output_folder)\n",
    "if not os.path.exists(split_bams_folder):\n",
    "    os.mkdir(split_bams_folder)\n",
    "\n",
    "num_contigs = 0\n",
    "\n",
    "all_bam_jobs = []\n",
    "\n",
    "for contig, df_dict in overall_label_to_list_of_contents.items():\n",
    "    num_contigs += 1\n",
    "    job_params = get_job_params(contig, df_dict, header_string)\n",
    "    for j in job_params:\n",
    "        all_bam_jobs.append(j)\n",
    "    \n",
    "overall_job_builder_time = time.perf_counter() - start_time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d100d6c4-3e3f-46a8-8bdf-6f6e45bad65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to prepare list for multiprocess-writing bams: 46 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"Total time to prepare list for multiprocess-writing bams: {} minutes\".format(round(overall_job_builder_time/60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd90c81-f322-46fb-965d-186f9f53630d",
   "metadata": {},
   "source": [
    "# Generate bams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3043e2d-862b-4221-ad44-2798515937e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#overall_label_to_list_of_contents.clear()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1025c628-9f9f-4600-bade-b292dbdd951e",
   "metadata": {},
   "source": [
    "all_bam_jobs = []\n",
    "for r in job_list:\n",
    "    for i in r:\n",
    "        all_bam_jobs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4896fa13-90e6-4ea6-b6d6-a8969c55053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_bam_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e15f9f3-65c1-4196-9855-dc1e86b5be2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_bam(bam_file_name):\n",
    "    output_name = bam_file_name + \".sorted.bam\"\n",
    "    pysam.sort(\"-o\", output_name, bam_file_name)  \n",
    "    return output_name\n",
    "\n",
    "\n",
    "def write_reads_to_file(reads, bam_file_name, header_string):\n",
    "    header = pysam.AlignmentHeader.from_text(header_string)\n",
    "    \n",
    "    header_dict = header.as_dict()\n",
    "    lengths_for_sn = {}\n",
    "    \n",
    "    header_dict_sq = header_dict.get(\"SQ\")\n",
    "    for s in header_dict_sq:\n",
    "        sn = s.get(\"SN\")\n",
    "        ln = s.get(\"LN\")\n",
    "        lengths_for_sn[sn] = ln\n",
    "        \n",
    "    print(\"\\tCurrent header length for {}: {}\".format(bam_file_name, len(lengths_for_sn)))\n",
    "    \n",
    "    all_barcodes_for_contig = set([r.split('\\t')[2] for r in reads])\n",
    "    print(\"\\tNum barcodes for {}: {}\".format(bam_file_name, len(all_barcodes_for_contig)))\n",
    "        \n",
    "    for new_sn in all_barcodes_for_contig:\n",
    "        new_sn_chrom = new_sn.split(\"_\")[0]\n",
    "        \n",
    "        new_ln = lengths_for_sn.get(new_sn_chrom)\n",
    "        new_entry = {\"SN\": new_sn, \"LN\": new_ln}\n",
    "        header_dict_sq.append(new_entry)\n",
    "    \n",
    "    #print(\"\\tExample new entries: {}\".format(header_dict_sq[-4:]))\n",
    "    header_dict['SQ'] = header_dict_sq\n",
    "    \n",
    "    print(\"\\tNew header length: {}\".format(len(header_dict.get(\"SQ\"))))\n",
    "    \n",
    "    new_header = pysam.AlignmentHeader.from_dict(header_dict)\n",
    "    \n",
    "    num_reads = len(reads)\n",
    "    \n",
    "    with pysam.AlignmentFile(bam_file_name, \"wb\", text=str(new_header)) as bam_handle:\n",
    "        for i, read_str in enumerate(reads):\n",
    "            if i % 100000 == 0:\n",
    "                print('file {}: {}/{} reads'.format(bam_file_name.split('/')[-1], i, num_reads))\n",
    "                \n",
    "            try:\n",
    "                read = pysam.AlignedSegment.fromstring(read_str, new_header)\n",
    "                bam_handle.write(read) \n",
    "            except Exception as e:\n",
    "                print('{}\\n\\nfile {}: Failed to write read with str representation of:\\n\\t {}'.format(e,\n",
    "                                                                                                      bam_file_name.split('/')[-1],\n",
    "                                                                                                read_str))\n",
    "                sys.exit(1)\n",
    "                \n",
    "            \n",
    "            \n",
    "    bam_handle.close()\n",
    "    \n",
    "            \n",
    "def write_reads_to_file_wrapper(parameters):\n",
    "    reads, bam_file_name, header_string = parameters\n",
    "    write_reads_to_file(reads, bam_file_name, header_string)\n",
    "    \n",
    "    try:\n",
    "        print(\"\\tSorting {}...\".format(bam_file_name))\n",
    "        sorted_bam_file_name = sort_bam(bam_file_name)\n",
    "        print(\"\\tIndexing {}...\".format(sorted_bam_file_name))\n",
    "        index_bam(sorted_bam_file_name)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Failed at indexing {}\".format(bam_file_name))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d728e6e-bb60-4996-a35e-346c8e62d867",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/76 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "with Pool(processes=16) as p:\n",
    "    max_ = len(all_bam_jobs)\n",
    "    with tqdm(total=max_) as pbar:\n",
    "        for _ in p.imap_unordered(write_reads_to_file_wrapper, all_bam_jobs):\n",
    "            pbar.update()\n",
    "\n",
    "total_bam_generation_time = time.perf_counter() - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0577cf-6600-4107-9db0-8399417d4c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total time to write bams: {} minutes\".format(round(total_bam_generation_time/60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e49151-ba3c-40a4-9290-3d8360753d96",
   "metadata": {},
   "source": [
    "# Second loop to get coverage at sites with edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab82bcb-c420-41d2-bd70-95f3958f54fc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "directory_path = os.path.abspath(os.path.join('../src/'))\n",
    "if directory_path not in sys.path:\n",
    "    sys.path.append(directory_path)\n",
    "    \n",
    "from utils import get_edit_info_for_barcode_in_contig_wrapper\n",
    "\n",
    "output_folder = '/projects/ps-yeolab3/ekofman/sailor2/scripts/full_test-highmem_bccontig'\n",
    "\n",
    "\n",
    "splits = [i.split(\"/\")[-1].split('_edit')[0] for i in glob('{}/edit_info/*'.format(output_folder))]\n",
    "print(\"Accessing split bams: {}\".format(', '.join(sorted(splits))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7b58ee-46a1-41e1-b3d5-d1aa4959975f",
   "metadata": {},
   "source": [
    "### Gather the edit information generated for each subcontig, and group by contig so we only have 1 edit information dataframe to process per contig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aa1ff3-69e7-4a5c-8cee-d6af8008eec0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_edit_info_for_barcodes = []\n",
    "\n",
    "edit_info_grouped_per_contig = defaultdict(lambda:[])\n",
    "edit_info_grouped_per_contig_combined = defaultdict(lambda:[])\n",
    "\n",
    "num_splits = len(splits)\n",
    "print(\"Grouping edit information outputs by contig...\")\n",
    "for i, split in enumerate(splits):\n",
    "    if i%10 == 0:\n",
    "        print(\"\\t{}/{}...\".format(i, num_splits))\n",
    "    contig = split.split(\"_\")[0]\n",
    "    \n",
    "    barcode_to_coverage_dict = defaultdict()    \n",
    "    \n",
    "    barcode_to_coverage_dict = defaultdict()\n",
    "    edit_info_file = '{}/edit_info/{}_edit_info.tsv'.format(output_folder, split)\n",
    "    edit_info_df = pd.read_csv(edit_info_file, sep='\\t')\n",
    "    edit_info_df['position'] = edit_info_df['position'].astype(int)\n",
    "    edit_info_df['base_quality'] = edit_info_df['base_quality'].astype(int)\n",
    "    edit_info_df['mapping_quality'] = edit_info_df['mapping_quality'].astype(int)\n",
    "    edit_info_df['dist_from_end'] = edit_info_df['dist_from_end'].astype(int)\n",
    "\n",
    "    edit_info = pl.from_pandas(edit_info_df) \n",
    "    \n",
    "    for n in [\"A\", \"C\", \"G\", \"T\"]:\n",
    "        suffix = '{}-1'.format(n)\n",
    "        edit_info_subset = edit_info.filter(pl.col(\"barcode\").str.ends_with(suffix))\n",
    "        \n",
    "        edit_info_grouped_per_contig[\"{}_{}\".format(contig, n)].append(edit_info_subset)\n",
    "    \n",
    "    del edit_info_df\n",
    "    \n",
    "print(\"Done grouping! Concatenating ...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b74e2a-dbc3-4b2f-82fc-bd40226f1b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for contig, list_of_edit_info_dfs in edit_info_grouped_per_contig.items():\n",
    "    edit_info_grouped_per_contig_combined[contig] = pl.concat(list_of_edit_info_dfs)\n",
    "\n",
    "print(\"Done concatenating!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ef51b-eeba-4fb5-a716-3d6cd204c1b6",
   "metadata": {},
   "source": [
    "### Get coverage at edit positions for each contig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a308e9a5-013e-4fa4-aa29-504abd72e80b",
   "metadata": {},
   "source": [
    "##### Merge across contigs for each barcode???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c22ae8-9b11-490a-8913-9fbe3fc7ff3e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "\n",
    "def get_job_params_for_coverage_for_edits_in_contig(edit_info_grouped_per_contig_combined, output_folder):\n",
    "    job_params = []\n",
    "    \n",
    "    for contig, edit_info in edit_info_grouped_per_contig_combined.items():\n",
    "        print(contig)\n",
    "        print('Num edits pre filter: {}'.format(len(edit_info)))\n",
    "        edit_info = edit_info.filter(pl.col(\"base_quality\") > 15)\n",
    "        print('\\tNum edits post filter: {}'.format(len(edit_info)))\n",
    "        \n",
    "        #unique_barcodes = list(edit_info.unique(\"barcode\")[\"barcode\"])\n",
    "        #unique_contigs = list(edit_info.unique(\"contig\")[\"contig\"])\n",
    "        #print('Num unique contigs: {}'.format(len(unique_contigs)))\n",
    "        \n",
    "        job_params.append([edit_info, contig, output_folder])  \n",
    "    return job_params\n",
    "    \n",
    "coverage_counting_job_params = get_job_params_for_coverage_for_edits_in_contig(edit_info_grouped_per_contig_combined, \n",
    "                                                                output_folder)\n",
    "len(coverage_counting_job_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0cf169-59d3-4c5e-857f-044d1693ddc5",
   "metadata": {},
   "source": [
    "### This is going at rate of 1 items per seconds... which would take several hours..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5392be-387b-40f6-b8ac-c684cec2bf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#edit_info_plus_coverage_df = get_edit_info_for_barcode_in_contig_wrapper(coverage_counting_job_params[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851efc7-2766-4666-b426-a1ab243db920",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from multiprocessing import get_context\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "results = []\n",
    "# Spawn has to be used instead of the default fork when using the polars library\n",
    "with get_context(\"spawn\").Pool(processes=16) as p:\n",
    "    max_ = len(coverage_counting_job_params)\n",
    "    with tqdm(total=max_) as pbar:\n",
    "        for _ in p.imap_unordered(get_edit_info_for_barcode_in_contig_wrapper, coverage_counting_job_params):\n",
    "            pbar.update()\n",
    "            results.append(_)\n",
    "            \n",
    "total_time = time.perf_counter() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd648239-8132-4ed1-b324-09e03b198a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7caa846-b0c5-4783-b7b7-984d8f8658ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edit_info = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaff01c-6a85-4ff6-a52c-6dd18c63f3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5647b4f-4dc5-40fc-b772-6007f8ccbfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edit_info.to_csv('{}/all_edit_info.tsv'.format(output_folder), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8620db3-658d-4e62-a352-302ce84c4c8c",
   "metadata": {},
   "source": [
    "# Group by site to get final total edit and coverage counts at each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a344ca-998d-415b-a508-42e9bd405d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edit_info.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03097c3-ad74-474b-8af6-921e9fc031e0",
   "metadata": {},
   "source": [
    "# Verify C>T ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999ddfc7-f83d-4cb8-afc3-2b0861674233",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edit_info.groupby(['ref', 'alt']).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803f5b24-4714-4309-a08b-9b9445218c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edit_info.groupby(['ref', 'alt']).count().plot(kind='barh', legend=False)\n",
    "plt.title(\"All edits\")\n",
    "\n",
    "base_quality_thresh = 15\n",
    "all_edit_info[all_edit_info.base_quality > base_quality_thresh].groupby(['ref', 'alt']).count().plot(kind='barh', legend=False)\n",
    "plt.title(\"Edits with base quality > {}\".format(base_quality_thresh))\n",
    "\n",
    "all_edit_info_filtered = all_edit_info[all_edit_info.base_quality > base_quality_thresh]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a97fbec-9329-4c9c-8c9b-a785a8ae376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_new_ct =  all_edit_info_filtered[(all_edit_info_filtered.ref == 'C') & (all_edit_info_filtered.alt == 'T')].sort_values('position')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b3fdc3-7fab-448f-a9e9-52cbab695e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(example_new_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be865e2-f294-4152-ae0b-85d24509a1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_new_ct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc30c7f-73aa-4be6-83a7-ff2a2072d602",
   "metadata": {},
   "source": [
    "# Cells that do have STAMP expressed versus don't...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7ee3ff-374f-4a6e-8ab7-0b1e1311c07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stamp_expression_path = \\\n",
    "'/projects/ps-yeolab3/ekofman/Sammi/MouseBrainEF1A_SingleCell_EPR_combined/\\\n",
    "4.1_cells_with_middling_stamp/stamp_expression_for_all_used_cells.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2684ed2-1146-4e4c-9b4d-186865d6e4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "stamp_expression_df = pd.read_csv(stamp_expression_path, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093153f4-7d3a-4a04-bf6d-ee855e23a02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stamp_expression_df.Stamp.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af056065-287e-4639-bc12-6f2e83468b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edit_info_filtered['edit'] = all_edit_info_filtered['ref'] + '>' + all_edit_info_filtered['alt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26edccde-cad7-48fe-ab64-20d91bcf8844",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "fractions_ct = []\n",
    "threshs = [0, 0.5, 1, 2, 3, 4, 5, 6, 6.5, 6.6]\n",
    "for thresh in threshs:\n",
    "    print(thresh)\n",
    "    barcodes_at_stamp_thresh = stamp_expression_df[stamp_expression_df.Stamp > thresh].index\n",
    "    \n",
    "    all_edit_info_filtered_in_stamp_level = all_edit_info_filtered[\n",
    "        all_edit_info_filtered.barcode.isin(barcodes_at_stamp_thresh)]\n",
    "    \n",
    "    all_edit_info_filtered_in_stamp_level.groupby(['ref', 'alt']).count().plot(kind='barh', legend=False)\n",
    "    plt.title(\"Edit Type Distribution for Cells with STAMP expression above {}\".format(thresh))\n",
    "    \n",
    "    fraction_ct = len(all_edit_info_filtered_in_stamp_level[all_edit_info_filtered_in_stamp_level['edit'] == 'C>T'])/len(all_edit_info_filtered_in_stamp_level)\n",
    "    fractions_ct.append(fraction_ct)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e6b127-1356-40c4-b700-02c31a0c5266",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(threshs, fractions_ct)\n",
    "plt.ylabel(\"Fraction of total edits that are C>T\")\n",
    "plt.xlabel(\"STAMP expression minimum\")\n",
    "plt.title(\"Enrichment for C>T edits within cells filtered by STAMP threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8800507-1d95-4bb6-94a8-9c0ec7de29fb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "fractions_ct_low = []\n",
    "threshs = [1, 2, 3, 4, 5, 6, 6.5, 6.6]\n",
    "for thresh in threshs:\n",
    "    print(thresh)\n",
    "    barcodes_at_stamp_thresh = stamp_expression_df[stamp_expression_df.Stamp < thresh].index\n",
    "    \n",
    "    all_edit_info_filtered_in_stamp_level = all_edit_info_filtered[\n",
    "        all_edit_info_filtered.barcode.isin(barcodes_at_stamp_thresh)]\n",
    "    \n",
    "    all_edit_info_filtered_in_stamp_level.groupby(['ref', 'alt']).count().plot(kind='barh', legend=False)\n",
    "    plt.title(\"Edit Type Distribution for Cells with STAMP expression below {}\".format(thresh))\n",
    "    \n",
    "    fraction_ct = len(all_edit_info_filtered_in_stamp_level[all_edit_info_filtered_in_stamp_level['edit'] == 'C>T'])/len(all_edit_info_filtered_in_stamp_level)\n",
    "    fractions_ct_low.append(fraction_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc326f-a695-41f6-bafb-5daf952a4dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(threshs, fractions_ct_low)\n",
    "plt.ylabel(\"Fraction of total edits that are C>T\")\n",
    "plt.xlabel(\"STAMP expression maximum\")\n",
    "plt.title(\"Enrichment for C>T edits within cells filtered by STAMP threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71e31ad-c7fb-4c3d-b1f2-84ed484cb525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_kernel",
   "language": "python",
   "name": "py38_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
