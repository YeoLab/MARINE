{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "834c9266-dd67-464d-b30d-eb56aee77724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam\n",
    "import os\n",
    "import sys\n",
    "from sys import getsizeof\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "directory_path = os.path.abspath(os.path.join('../src/'))\n",
    "if directory_path not in sys.path:\n",
    "    sys.path.append(directory_path)\n",
    "    \n",
    "from read_process import get_contig_lengths_dict,\\\n",
    "incorporate_replaced_pos_info,incorporate_insertions_and_deletions,\\\n",
    "get_positions_from_md_tag,reverse_complement,get_edit_information,get_edit_information_wrapper,\\\n",
    "has_edits,get_total_coverage_for_contig_at_position,\\\n",
    "print_read_info, update_coverage_array, get_read_information, get_hamming_distance, remove_softclipped_bases,find\n",
    "\n",
    "from utils import get_intervals, index_bam, write_rows_to_info_file, write_header_to_bam, \\\n",
    "write_read_to_bam_file, remove_file_if_exists, make_folder\n",
    "\n",
    "import os, psutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a51607-e768-4314-939b-d59068863e5d",
   "metadata": {},
   "source": [
    "# Preload which barcodes to use..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a707e5c-8b77-4952-91af-c3f90802dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "barcodes_list_path = '/projects/ps-yeolab3/ekofman/Sammi/MouseBrainEF1A_SingleCell_EPR_batch2/cellranger/results/ms_hippo_stamp_EIF4A_batch2/outs/filtered_feature_bc_matrix/barcodes.tsv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09215472-0794-47c5-a306-5461de94d169",
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode_whitelist = set(pd.read_csv(barcodes_list_path, names=['barcodes']).barcodes.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b41cb49-f4b4-464f-ae2b-19d808f210e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39200"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(barcode_whitelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72010a82-87a7-4198-aa66-79f57ad70337",
   "metadata": {},
   "source": [
    "# ~~~~~~~~~~~~~~~~~~\n",
    "# Multi-processing enabled\n",
    "# ~~~~~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7166d4-3f54-4108-abb8-e978f704727a",
   "metadata": {},
   "source": [
    "# An example on a full 10x bam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5962296-f5be-4c0f-8323-74f591d58dea",
   "metadata": {},
   "source": [
    "#### in 10X's bam file, xf=25 means that read is uniquely mapped to a genome, and was used for counting UMI. So we should only look at reads with xf=25 from the 10X bam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fba3a6f1-60ab-4ec3-9f17-a59ab8d4bbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bampath = '/projects/ps-yeolab5/ekofman/Sammi/MouseBrainEF1A_SingleCell_EPR_batch2/filtered_possorted_ms_hippo_stamp_bam/filtered_keep_xf25_possorted_genome_with_header.bam_MD.bam'\n",
    "bampath = '/projects/ps-yeolab3/ekofman/sailor2/data/groups_0_1_2_3_4_5_6_7_8_9_10_11_merged.bam'\n",
    "\n",
    "\n",
    "samfile = pysam.AlignmentFile(bampath, \"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "606494dd-4fd1-4760-9ad7-2c201a19e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "samfile_header = str(samfile.header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cec4616a-69eb-477c-a072-b15d570369d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19338.323"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getsizeof(samfile_header)/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ea541e-ab33-40a6-9d12-88c707c3932c",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "842901f9-7341-4c1f-87c7-0dd89cd8ac8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_edits(bampath, contig, split_index, start, end, output_folder, barcode_whitelist=None, verbose=False):  \n",
    "    time_reporting = {}\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    samfile = pysam.AlignmentFile(bampath, \"rb\")\n",
    "        \n",
    "    counts = defaultdict(lambda:defaultdict(lambda:0))\n",
    "    total_reads = 0\n",
    "    \n",
    "    bam_handles_for_barcodes = {}\n",
    "    read_lists_for_barcodes = defaultdict(lambda:[])\n",
    "    \n",
    "    reads_for_contig = samfile.fetch(contig, start, end, multiple_iterators=True)\n",
    "\n",
    "    output_file = '{}/{}_{}_{}_{}_edit_info.tsv'.format(edit_info_subfolder, contig, split_index, start, end)\n",
    "    remove_file_if_exists(output_file)\n",
    "\n",
    "    with open(output_file, 'w') as f:        \n",
    "        write_header_to_bam(f)\n",
    "\n",
    "        for i, read in enumerate(reads_for_contig):\n",
    "            total_reads += 1\n",
    "            \n",
    "            if total_reads % 1000 == 0:\n",
    "                time_reporting[total_reads] = time.perf_counter() - start_time\n",
    "\n",
    "            barcode = read.get_tag(\"CB\")\n",
    "            if barcode_whitelist:\n",
    "                if barcode not in barcode_whitelist:\n",
    "                    counts[contig]['Barcode Filtered'] += 1\n",
    "                    continue\n",
    "                \n",
    "            barcodes[contig][barcode] += 1\n",
    "\n",
    "            verbose = False\n",
    "            \n",
    "            try:\n",
    "                error_code, list_of_rows, num_edits_of_each_type = get_read_information(read, contig, verbose=verbose)\n",
    "            except Exception as e:\n",
    "                print(\"Failed on\\n{}\".format(read.to_string()))\n",
    "                break\n",
    "                \n",
    "            if error_code:\n",
    "                counts[contig][error_code] += 1\n",
    "            else:\n",
    "                counts[contig][EDITED_CODE] += 1\n",
    "                write_rows_to_info_file(list_of_rows, f)\n",
    "            \n",
    "            # Store each read using its string representation\n",
    "            read_as_string = read.to_string()\n",
    "                    \n",
    "            read_lists_for_barcodes[barcode].append(read_as_string)\n",
    "            \n",
    "    \n",
    "    # Add all reads to dictionary for contig and barcode, in their string representation\n",
    "    num_barcodes = 0\n",
    "    total_bams = len(read_lists_for_barcodes)\n",
    "    \n",
    "    \n",
    "    barcode_to_concatted_reads = {}\n",
    "    for barcode, read_list in read_lists_for_barcodes.items():\n",
    "        num_barcodes += 1\n",
    "        if num_barcodes % 100 == 0:\n",
    "            #print('{}/{} processed'.format(num_barcodes, total_bams))\n",
    "            pass\n",
    "        # Concatenate the string representations of all reads for each bam-contig combination\n",
    "        all_reads_concatted = '\\n'.join(read_list)\n",
    "            \n",
    "        # Save this concatenated block of text to dictionary\n",
    "        barcode_to_concatted_reads[barcode] = all_reads_concatted\n",
    "        \n",
    "    time_reporting[total_reads] = time.perf_counter() - start_time\n",
    "    \n",
    "    samfile.close()\n",
    "    \n",
    "    return barcode_to_concatted_reads, total_reads, barcodes, counts, time_reporting\n",
    "\n",
    "\n",
    "def find_edits_and_split_bams(bampath, contig, split_index, start, end, output_folder, barcode_whitelist=None, verbose=False):\n",
    "    barcode_to_concatted_reads, total_reads, barcodes, counts, time_reporting = find_edits(bampath, contig, split_index,\n",
    "                                                                         start, end, output_folder, barcode_whitelist=barcode_whitelist, verbose=verbose)    \n",
    "    return barcode_to_concatted_reads, total_reads, barcodes, counts, time_reporting\n",
    "    \n",
    "def find_edits_and_split_bams_wrapper(parameters):\n",
    "    try:\n",
    "        start_time = time.perf_counter()\n",
    "        bampath, contig, split_index, start, end, output_folder, barcode_whitelist, verbose = parameters\n",
    "        label = '{}({}):{}-{}'.format(contig, split_index, start, end)\n",
    "\n",
    "        #print(\"{} ({}):{}-{}\\tfind_edits_and_split_bams\".format(contig, split_index, start, end))\n",
    "        barcode_to_concatted_reads, total_reads, barcodes, counts, time_reporting = find_edits_and_split_bams(bampath, contig, split_index, start, end,                                                                                        \n",
    "                                                                                                              output_folder, \n",
    "                                                                                                              barcode_whitelist=barcode_whitelist,\n",
    "                                                                                                              verbose=False)\n",
    "        barcodes_df = pd.DataFrame.from_dict(barcodes)\n",
    "        counts_df = pd.DataFrame.from_dict(counts)\n",
    "        time_df = pd.DataFrame.from_dict(time_reporting, orient='index')\n",
    "        barcode_to_concatted_reads_df = pd.DataFrame.from_dict(barcode_to_concatted_reads, orient='index')\n",
    "        \n",
    "        total_time = time.perf_counter() - start_time\n",
    "        return barcode_to_concatted_reads_df, total_reads, barcodes_df, label, counts_df, time_df, total_time\n",
    "    except Exception as e:\n",
    "        print('Contig {}: {}'.format(label, e))\n",
    "        return 0, pd.DataFrame(), label, pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e80d3e0-398f-49e2-bf55-281a9621698f",
   "metadata": {},
   "source": [
    "# Go through every read and identify all edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5481bde1-a0e7-43b6-8699-1043a6d7cae3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU count: 36\n",
      "Contig 1\n",
      "Contig 10\n",
      "Contig 11\n",
      "Contig 12\n",
      "Contig 13\n",
      "Contig 14\n",
      "Contig 15\n",
      "Contig 16\n",
      "Contig 17\n",
      "Contig 18\n",
      "Contig 19\n",
      "Contig 2\n",
      "Contig 3\n",
      "Contig 4\n",
      "Contig 5\n",
      "Contig 6\n",
      "Contig 7\n",
      "Contig 8\n",
      "Contig 9\n",
      "Contig MT\n",
      "Contig X\n",
      "Contig Y\n",
      "352 total jobs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 352/352 [01:40<00:00,  3.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "#from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "print(\"CPU count: {}\".format(multiprocessing.cpu_count()))\n",
    "\n",
    "output_folder = '/projects/ps-yeolab3/ekofman/sailor2/scripts/full_test-highmem'\n",
    "\n",
    "contig_lengths_dict = get_contig_lengths_dict(samfile)\n",
    "\n",
    "# Print info?\n",
    "verbose = False \n",
    "EDITED_CODE = 'edited'\n",
    "\n",
    "# How many subcontigs to split each contig into to leverage multi-processing\n",
    "num_intervals = 16\n",
    "\n",
    "num_reads_to_coverage_dict_kb = {}\n",
    "num_reads_to_seconds = {}\n",
    "\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "total_seconds_for_reads = {0: 1}\n",
    "\n",
    "barcodes = defaultdict(lambda:defaultdict(lambda:0))\n",
    "\n",
    "jobs = []\n",
    "for contig in contig_lengths_dict.keys():\n",
    "    # Skip useless contigs\n",
    "    if len(contig) > 5 or contig == 'Stamp':# or contig != '17':\n",
    "        continue\n",
    "        \n",
    "    print(\"Contig {}\".format(contig))\n",
    "    contig_length = contig_lengths_dict.get(contig)\n",
    "    intervals_for_contig = get_intervals(contig, contig_lengths_dict, num_intervals)\n",
    "    \n",
    "    # Make subfolder in which to information about edits\n",
    "    edit_info_subfolder = '{}/edit_info'.format(output_folder)\n",
    "    make_folder(edit_info_subfolder)\n",
    "        \n",
    "    # Set up for pool\n",
    "    for split_index, interval in enumerate(intervals_for_contig):\n",
    "        split_index = str(split_index).zfill(3)\n",
    "        parameters = [bampath, contig, split_index, interval[0], interval[1], output_folder, barcode_whitelist, verbose]\n",
    "        jobs.append(parameters)\n",
    "    \n",
    "print(\"{} total jobs\".format(len(jobs)))\n",
    "\n",
    "# Pooling\n",
    "results = []\n",
    "overall_total_reads = 0\n",
    "with Pool(processes=6) as p:\n",
    "    max_ = len(jobs)\n",
    "    with tqdm(total=max_) as pbar:\n",
    "        for _ in p.imap_unordered(find_edits_and_split_bams_wrapper, jobs):\n",
    "            pbar.update()\n",
    "            results.append(_)\n",
    "            \n",
    "            total_reads = _[1]\n",
    "            total_time = time.perf_counter() - start_time\n",
    "            \n",
    "            overall_total_reads += total_reads\n",
    "\n",
    "            total_seconds_for_reads[overall_total_reads] = total_time\n",
    "\n",
    "overall_time = time.perf_counter() - start_time \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df0f1977-a6e9-4653-97ef-fb2afa07ad19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 101.14009240269661 seconds\n",
      "Total time: 1.6856682067116102 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"Total time: {} seconds\".format(overall_time))\n",
    "print(\"Total time: {} minutes\".format(overall_time/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74036ccf-01c5-49d3-8bcd-6bbca94f8a49",
   "metadata": {},
   "source": [
    "Memory: 345 Gigabytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7ad726-4045-4229-ae90-369ae6327f06",
   "metadata": {},
   "source": [
    "# More helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75188cf5-e1f2-4390-ba25-541e0f7b2800",
   "metadata": {},
   "source": [
    "# Combine all of the reads (string representation) for each barcode\n",
    "## Groups the results from each sub-contig segment above, for example the reads from the first half of chr1 and those from the second half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7830a72a-38bb-4e9a-ab7a-fe3867daf56a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length mismatch: Expected axis has 0 elements, new values have 1 elements MT(000):0-1019\n",
      "Length mismatch: Expected axis has 0 elements, new values have 1 elements MT(001):1019-2038\n",
      "Length mismatch: Expected axis has 0 elements, new values have 1 elements Y(004):22936176-28670220\n",
      "Length mismatch: Expected axis has 0 elements, new values have 1 elements Y(005):28670220-34404264\n",
      "Length mismatch: Expected axis has 0 elements, new values have 1 elements Y(008):45872352-51606396\n",
      "Length mismatch: Expected axis has 0 elements, new values have 1 elements Y(009):51606396-57340440\n",
      "Length mismatch: Expected axis has 0 elements, new values have 1 elements Y(012):68808528-74542572\n",
      "Length mismatch: Expected axis has 0 elements, new values have 1 elements Y(014):80276616-86010660\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "edit_finder_results = results\n",
    "\n",
    "num_barcodes_to_time = {}\n",
    "num_contigs_to_time = {}\n",
    "\n",
    "overall_label_to_list_of_contents = defaultdict(lambda:{})\n",
    "\n",
    "for barcode_to_concatted_reads_df, total_reads, barcodes_df, label, counts_df, time_df, total_time in edit_finder_results:\n",
    "    try:\n",
    "        \n",
    "        if len(barcode_to_concatted_reads_df.columns) < 3:\n",
    "            barcode_to_concatted_reads_df.columns = ['contents']\n",
    "            barcode_to_concatted_reads_df['barcode'] = [b.split('/')[-1].split('.bam')[0] for b in barcode_to_concatted_reads_df.index]\n",
    "            contig = label.split('(')[0]\n",
    "            barcode_to_concatted_reads_df['barcode_contig'] = barcode_to_concatted_reads_df['barcode'] + '_' + contig\n",
    "                   \n",
    "        overall_label_to_list_of_contents[contig][label] = pl.from_pandas(barcode_to_concatted_reads_df)\n",
    "    except Exception as e:\n",
    "        print(e, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de955557-09d6-4d68-b415-8e16802c64ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall contigs:\n",
      "\n",
      "\t dict_keys(['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '3', '4', '5', '6', '7', '8', '9', 'MT', 'X', 'Y'])\n",
      "\n",
      "Subcontig regions for an example contig (1):\n",
      "\n",
      "\t ['1(000):0-12216999', '1(001):12216999-24433998', '1(002):24433998-36650997', '1(003):36650997-48867996', '1(004):48867996-61084995', '1(005):61084995-73301994', '1(006):73301994-85518993', '1(007):85518993-97735992', '1(008):97735992-109952991', '1(009):109952991-122169990', '1(010):122169990-134386989', '1(011):134386989-146603988', '1(012):146603988-158820987', '1(013):158820987-171037986', '1(014):171037986-183254985', '1(015):183254985-195471984']\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall contigs:\\n\\n\\t\", overall_label_to_list_of_contents.keys())\n",
    "print(\"\\nSubcontig regions for an example contig (1):\\n\\n\\t\",sorted(overall_label_to_list_of_contents['1'].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8134fb0e-6dd2-4348-9848-ee1cf666cd3b",
   "metadata": {},
   "source": [
    "### Generate list of jobs to be multiprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59781d69-3789-4528-b509-9b529b5ad199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_params(contig, df_dict):\n",
    "    job_params = []\n",
    "    # Make a sub-subfolder to put the bams for this specific contig\n",
    "    contig_folder = '{}/{}/'.format(split_bams_folder, contig)\n",
    "    if not os.path.exists(contig_folder):\n",
    "        os.mkdir(contig_folder)\n",
    "    \n",
    "    # Sort the subcontig regions such that the reads are properly ordered \n",
    "    sorted_subcontig_names = sorted(df_dict.keys())\n",
    "    sorted_subcontig_dfs = []\n",
    "    for n in sorted_subcontig_names:\n",
    "        sorted_subcontig_dfs.append(df_dict.get(n))\n",
    "        \n",
    "    print(\"\\t{}: num subcontigs to concat: {}\".format(contig, len(sorted_subcontig_dfs)))\n",
    "    # All of the reads for all of the barcodes are in this dataframe\n",
    "    print(\"\\t{}: concatting\".format(contig))\n",
    "    all_contents_df = pl.concat(sorted_subcontig_dfs)\n",
    "    \n",
    "         \n",
    "    all_contents_df_partitioned = all_contents_df.partition_by(\"barcode\")\n",
    "    total_barcodes = len(all_contents_df_partitioned)\n",
    "    \n",
    "    for i, contents_for_barcode in enumerate(all_contents_df_partitioned):            \n",
    "        if i % 10000 == 0:\n",
    "            print('Contig {}: {}/{} barcodes'.format(contig, i, total_barcodes))\n",
    "\n",
    "        # Combine the reads (in string representation) for all rows corresponding to a barcode        \n",
    "        all_reads_for_barcode_concatted = contents_for_barcode.transpose().with_columns(\n",
    "            pl.concat_str(\n",
    "                [pl.col(c) for c in contents_for_barcode.transpose().columns],\n",
    "                separator=\"\\n\"\n",
    "                 ).alias(\"combined_text\")\n",
    "        )[['combined_text']][0].item()\n",
    "            \n",
    "        # Turn the newline-delimited block of text back into list of reads as strings\n",
    "        # OrderedDict line is to remove duplicates\n",
    "        reads_deduped = list(OrderedDict.fromkeys(all_reads_for_barcode_concatted.split('\\n')))\n",
    "                \n",
    "        # Establish the name of the split bam that will be generated\n",
    "        barcode = contents_for_barcode.unique('barcode')['barcode'].item()\n",
    "        bam_file_name = '{}/{}_{}.bam'.format(contig_folder, contig, barcode)\n",
    "        \n",
    "        # Add parameters to list of jobs\n",
    "        job_params.append([reads_deduped, bam_file_name, header_string])\n",
    "    return job_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6589a91c-43b3-44eb-9a06-ae430dba3c3b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1: num subcontigs to concat: 16\n",
      "\t1: concatting\n",
      "Contig 1: 0/1500 barcodes\n",
      "\t10: num subcontigs to concat: 16\n",
      "\t10: concatting\n",
      "Contig 10: 0/1500 barcodes\n",
      "\t11: num subcontigs to concat: 16\n",
      "\t11: concatting\n",
      "Contig 11: 0/1500 barcodes\n",
      "\t12: num subcontigs to concat: 16\n",
      "\t12: concatting\n",
      "Contig 12: 0/1500 barcodes\n",
      "\t13: num subcontigs to concat: 16\n",
      "\t13: concatting\n",
      "Contig 13: 0/1500 barcodes\n",
      "\t14: num subcontigs to concat: 16\n",
      "\t14: concatting\n",
      "Contig 14: 0/1500 barcodes\n",
      "\t15: num subcontigs to concat: 16\n",
      "\t15: concatting\n",
      "Contig 15: 0/1500 barcodes\n",
      "\t16: num subcontigs to concat: 16\n",
      "\t16: concatting\n",
      "Contig 16: 0/1500 barcodes\n",
      "\t17: num subcontigs to concat: 16\n",
      "\t17: concatting\n",
      "Contig 17: 0/1500 barcodes\n",
      "\t18: num subcontigs to concat: 16\n",
      "\t18: concatting\n",
      "Contig 18: 0/1500 barcodes\n",
      "\t19: num subcontigs to concat: 16\n",
      "\t19: concatting\n",
      "Contig 19: 0/1500 barcodes\n",
      "\t2: num subcontigs to concat: 16\n",
      "\t2: concatting\n",
      "Contig 2: 0/1500 barcodes\n",
      "\t3: num subcontigs to concat: 16\n",
      "\t3: concatting\n",
      "Contig 3: 0/1500 barcodes\n",
      "\t4: num subcontigs to concat: 16\n",
      "\t4: concatting\n",
      "Contig 4: 0/1500 barcodes\n",
      "\t5: num subcontigs to concat: 16\n",
      "\t5: concatting\n",
      "Contig 5: 0/1500 barcodes\n",
      "\t6: num subcontigs to concat: 16\n",
      "\t6: concatting\n",
      "Contig 6: 0/1500 barcodes\n",
      "\t7: num subcontigs to concat: 16\n",
      "\t7: concatting\n",
      "Contig 7: 0/1500 barcodes\n",
      "\t8: num subcontigs to concat: 16\n",
      "\t8: concatting\n",
      "Contig 8: 0/1500 barcodes\n",
      "\t9: num subcontigs to concat: 16\n",
      "\t9: concatting\n",
      "Contig 9: 0/1500 barcodes\n",
      "\tMT: num subcontigs to concat: 14\n",
      "\tMT: concatting\n",
      "Contig MT: 0/1499 barcodes\n",
      "\tX: num subcontigs to concat: 16\n",
      "\tX: concatting\n",
      "Contig X: 0/1500 barcodes\n",
      "\tY: num subcontigs to concat: 10\n",
      "\tY: concatting\n",
      "Contig Y: 0/1063 barcodes\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "# Get the bam header, which will be used for each of the split bams too\n",
    "header_string = str(samfile.header)\n",
    "\n",
    "# Make a subfolder into which the split bams will be placed\n",
    "split_bams_folder = '{}/split_bams'.format(output_folder)\n",
    "if not os.path.exists(split_bams_folder):\n",
    "    os.mkdir(split_bams_folder)\n",
    "\n",
    "num_contigs = 0\n",
    "\n",
    "job_list = []\n",
    "\n",
    "for contig, df_dict in overall_label_to_list_of_contents.items():\n",
    "    num_contigs += 1\n",
    "    job_params = get_job_params(contig, df_dict)\n",
    "    job_list.append(job_params)\n",
    "    \n",
    "overall_job_builder_time = time.perf_counter() - start_time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d100d6c4-3e3f-46a8-8bdf-6f6e45bad65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to prepare list for multiprocess-writing bams: 1 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"Total time to prepare list for multiprocess-writing bams: {} minutes\".format(round(overall_job_builder_time/60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd90c81-f322-46fb-965d-186f9f53630d",
   "metadata": {},
   "source": [
    "# Generate bams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3043e2d-862b-4221-ad44-2798515937e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_label_to_list_of_contents.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8e496d7-284a-43ed-88e8-72cef4b175f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(job_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d45e2dc0-ecd2-4f57-b509-f72618c24ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bam_jobs = []\n",
    "for r in job_list:\n",
    "    for i in r:\n",
    "        all_bam_jobs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4896fa13-90e6-4ea6-b6d6-a8969c55053b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32562"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_bam_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1e15f9f3-65c1-4196-9855-dc1e86b5be2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_bam(bam_file_name):\n",
    "    output_name = bam_file_name.split(\"bam\")[0] + \".sorted.bam\"\n",
    "    pysam.sort(\"-o\", output_name, bam_file_name)  \n",
    "    return output_name\n",
    "\n",
    "def write_reads_to_file(reads, bam_file_name, header_string):\n",
    "    with pysam.AlignmentFile(bam_file_name, \"wb\", text=header_string) as bam_handle:\n",
    "        for read_str in reads:\n",
    "            read = pysam.AlignedSegment.fromstring(read_str, bam_handle.header)\n",
    "            bam_handle.write(read) \n",
    "    bam_handle.close()\n",
    "            \n",
    "def write_reads_to_file_wrapper(parameters):\n",
    "    reads, bam_file_name, header_string = parameters\n",
    "    write_reads_to_file(reads, bam_file_name, header_string)\n",
    "    \n",
    "    try:\n",
    "        index_bam(bam_file_name)\n",
    "    except Exception as e:\n",
    "        print(\"Failed at indexing {}\".format(bam_file_name))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d728e6e-bb60-4996-a35e-346c8e62d867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 32562/32562 [19:35<00:00, 27.69it/s]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "with Pool(processes=16) as p:\n",
    "    max_ = len(all_bam_jobs)\n",
    "    with tqdm(total=max_) as pbar:\n",
    "        for _ in p.imap_unordered(write_reads_to_file_wrapper, all_bam_jobs):\n",
    "            pbar.update()\n",
    "\n",
    "total_time = time.perf_counter() - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a0577cf-6600-4107-9db0-8399417d4c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to write bams: 20 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"Total time to write bams: {} minutes\".format(round(total_time/60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa2d81-c8bc-4bf9-9487-b67bea3ec0eb",
   "metadata": {},
   "source": [
    "# Time profiling of the edit-counting step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84dae60-ee51-4358-9784-596af7449002",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_contig_times = {}\n",
    "all_read_info_dfs = []\n",
    "all_time_dfs = []\n",
    "\n",
    "total_times = {}\n",
    "for result in results:\n",
    "    \n",
    "    label = result[3]\n",
    "\n",
    "    try:\n",
    "        total_time = result[6]\n",
    "        total_times[label] = total_time\n",
    "        \n",
    "        time_df = result[5]\n",
    "        all_time_dfs.append(time_df)\n",
    "        total_time_for_contig = (float(time_df.max()))\n",
    "        total_contig_times[label] = total_time_for_contig\n",
    "        \n",
    "        read_info_df = result[4]\n",
    "        read_info_df.columns = [label]\n",
    "        all_read_info_dfs.append(read_info_df)\n",
    "    except Exception as e:\n",
    "        print(e, label)\n",
    "        \n",
    "total_contig_times_df = pd.DataFrame.from_dict(total_contig_times, orient='index', columns=['seconds']).sort_values('seconds')\n",
    "print('Total time without threading: {} minutes'.format(round(total_contig_times_df.seconds.sum()/60, 2)))\n",
    "\n",
    "total_reads_df = pd.concat(all_read_info_dfs,axis=1).T\n",
    "\n",
    "total_reads_and_times_df = total_reads_df.join(total_contig_times_df)\n",
    "\n",
    "\n",
    "# Rates\n",
    "\n",
    "rates = []\n",
    "for reads, secs in zip(list(total_seconds_for_reads.keys()), list(total_seconds_for_reads.values())):\n",
    "    rate = reads/secs\n",
    "    rates.append(rate)\n",
    "average_rate = np.mean(rates)\n",
    "print(\"Average of {} reads/second\".format(average_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc54c741-74a2-40bf-9d9f-aef31b084c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(total_reads_and_times_df.edited, total_reads_and_times_df.seconds, s=1)\n",
    "plt.scatter(total_reads_and_times_df.no_edits, total_reads_and_times_df.seconds, s=1)\n",
    "\n",
    "plt.title(\"Total processing time vs number of reads\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.xlabel(\"Reads\")\n",
    "plt.legend(['Reads with edits', 'Read without edits'])\n",
    "\n",
    "pd.DataFrame.from_dict(total_seconds_for_reads, orient='index').sort_index().plot(legend=False)\n",
    "plt.xlabel(\"Reads\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.title(\"Runtime vs number of reads processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202d2ff9-8a63-49e0-ab56-678d89cebb99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76b2337-3172-4383-a63f-eedfea506871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22692d03-d308-44b2-8dd3-ed39c20b6166",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(rates)), rates)\n",
    "plt.title(\"Mean rate (reads per second)\")\n",
    "plt.ylabel(\"Reads/Second\")\n",
    "plt.xlabel(\"Number of reads (e10^6)\")\n",
    "plt.axhline(average_rate, color='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44894b19-e631-46ac-ad91-f9dbac3d91fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds_per_read = 1/average_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d0c4d3-f985-4275-8a78-1a176188eae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "reads_per_cell = 50000\n",
    "total_cells = 15000\n",
    "total_reads = reads_per_cell * total_cells\n",
    "print(total_reads)\n",
    "\n",
    "total_estimated_time = total_reads * seconds_per_read\n",
    "print('Estimated total time in minutes for {} cells with {} reads per cell ({} total reads): {} minutes'.format(total_cells,\n",
    "                                                                                               reads_per_cell,\n",
    "                                                                                                                total_reads,\n",
    "                                                                                               math.ceil(total_estimated_time/60), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e49151-ba3c-40a4-9290-3d8360753d96",
   "metadata": {},
   "source": [
    "# Second loop to get coverage at sites with edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ab82bcb-c420-41d2-bd70-95f3958f54fc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing split bams: 10_000_0_8168438, 10_001_8168438_16336876, 10_002_16336876_24505314, 10_003_24505314_32673752, 10_004_32673752_40842190, 10_005_40842190_49010628, 10_006_49010628_57179066, 10_007_57179066_65347504, 10_008_65347504_73515942, 10_009_73515942_81684380, 10_010_81684380_89852818, 10_011_89852818_98021256, 10_012_98021256_106189694, 10_013_106189694_114358132, 10_014_114358132_122526570, 10_015_122526570_130695008, 11_000_0_7630159, 11_001_7630159_15260318, 11_002_15260318_22890477, 11_003_22890477_30520636, 11_004_30520636_38150795, 11_005_38150795_45780954, 11_006_45780954_53411113, 11_007_53411113_61041272, 11_008_61041272_68671431, 11_009_68671431_76301590, 11_010_76301590_83931749, 11_011_83931749_91561908, 11_012_91561908_99192067, 11_013_99192067_106822226, 11_014_106822226_114452385, 11_015_114452385_122082544, 12_000_0_7508064, 12_001_7508064_15016128, 12_002_15016128_22524192, 12_003_22524192_30032256, 12_004_30032256_37540320, 12_005_37540320_45048384, 12_006_45048384_52556448, 12_007_52556448_60064512, 12_008_60064512_67572576, 12_009_67572576_75080640, 12_010_75080640_82588704, 12_011_82588704_90096768, 12_012_90096768_97604832, 12_013_97604832_105112896, 12_014_105112896_112620960, 12_015_112620960_120129024, 13_000_0_7526353, 13_001_7526353_15052706, 13_002_15052706_22579059, 13_003_22579059_30105412, 13_004_30105412_37631765, 13_005_37631765_45158118, 13_006_45158118_52684471, 13_007_52684471_60210824, 13_008_60210824_67737177, 13_009_67737177_75263530, 13_010_75263530_82789883, 13_011_82789883_90316236, 13_012_90316236_97842589, 13_013_97842589_105368942, 13_014_105368942_112895295, 13_015_112895295_120421648, 14_000_0_7806391, 14_001_7806391_15612782, 14_002_15612782_23419173, 14_003_23419173_31225564, 14_004_31225564_39031955, 14_005_39031955_46838346, 14_006_46838346_54644737, 14_007_54644737_62451128, 14_008_62451128_70257519, 14_009_70257519_78063910, 14_010_78063910_85870301, 14_011_85870301_93676692, 14_012_93676692_101483083, 14_013_101483083_109289474, 14_014_109289474_117095865, 14_015_117095865_124902256, 15_000_0_6502731, 15_001_6502731_13005462, 15_002_13005462_19508193, 15_003_19508193_26010924, 15_004_26010924_32513655, 15_005_32513655_39016386, 15_006_39016386_45519117, 15_007_45519117_52021848, 15_008_52021848_58524579, 15_009_58524579_65027310, 15_010_65027310_71530041, 15_011_71530041_78032772, 15_012_78032772_84535503, 15_013_84535503_91038234, 15_014_91038234_97540965, 15_015_97540965_104043696, 16_000_0_6137986, 16_001_6137986_12275972, 16_002_12275972_18413958, 16_003_18413958_24551944, 16_004_24551944_30689930, 16_005_30689930_36827916, 16_006_36827916_42965902, 16_007_42965902_49103888, 16_008_49103888_55241874, 16_009_55241874_61379860, 16_010_61379860_67517846, 16_011_67517846_73655832, 16_012_73655832_79793818, 16_013_79793818_85931804, 16_014_85931804_92069790, 16_015_92069790_98207776, 17_000_0_5936705, 17_001_5936705_11873410, 17_002_11873410_17810115, 17_003_17810115_23746820, 17_004_23746820_29683525, 17_005_29683525_35620230, 17_006_35620230_41556935, 17_007_41556935_47493640, 17_008_47493640_53430345, 17_009_53430345_59367050, 17_010_59367050_65303755, 17_011_65303755_71240460, 17_012_71240460_77177165, 17_013_77177165_83113870, 17_014_83113870_89050575, 17_015_89050575_94987280, 18_000_0_5668915, 18_001_5668915_11337830, 18_002_11337830_17006745, 18_003_17006745_22675660, 18_004_22675660_28344575, 18_005_28344575_34013490, 18_006_34013490_39682405, 18_007_39682405_45351320, 18_008_45351320_51020235, 18_009_51020235_56689150, 18_010_56689150_62358065, 18_011_62358065_68026980, 18_012_68026980_73695895, 18_013_73695895_79364810, 18_014_79364810_85033725, 18_015_85033725_90702640, 19_000_0_3839473, 19_001_3839473_7678946, 19_002_7678946_11518419, 19_003_11518419_15357892, 19_004_15357892_19197365, 19_005_19197365_23036838, 19_006_23036838_26876311, 19_007_26876311_30715784, 19_008_30715784_34555257, 19_009_34555257_38394730, 19_010_38394730_42234203, 19_011_42234203_46073676, 19_012_46073676_49913149, 19_013_49913149_53752622, 19_014_53752622_57592095, 19_015_57592095_61431568, 1_000_0_12216999, 1_001_12216999_24433998, 1_002_24433998_36650997, 1_003_36650997_48867996, 1_004_48867996_61084995, 1_005_61084995_73301994, 1_006_73301994_85518993, 1_007_85518993_97735992, 1_008_97735992_109952991, 1_009_109952991_122169990, 1_010_122169990_134386989, 1_011_134386989_146603988, 1_012_146603988_158820987, 1_013_158820987_171037986, 1_014_171037986_183254985, 1_015_183254985_195471984, 2_000_0_11382077, 2_001_11382077_22764154, 2_002_22764154_34146231, 2_003_34146231_45528308, 2_004_45528308_56910385, 2_005_56910385_68292462, 2_006_68292462_79674539, 2_007_79674539_91056616, 2_008_91056616_102438693, 2_009_102438693_113820770, 2_010_113820770_125202847, 2_011_125202847_136584924, 2_012_136584924_147967001, 2_013_147967001_159349078, 2_014_159349078_170731155, 2_015_170731155_182113232, 3_000_0_10002480, 3_001_10002480_20004960, 3_002_20004960_30007440, 3_003_30007440_40009920, 3_004_40009920_50012400, 3_005_50012400_60014880, 3_006_60014880_70017360, 3_007_70017360_80019840, 3_008_80019840_90022320, 3_009_90022320_100024800, 3_010_100024800_110027280, 3_011_110027280_120029760, 3_012_120029760_130032240, 3_013_130032240_140034720, 3_014_140034720_150037200, 3_015_150037200_160039680, 4_000_0_9781758, 4_001_9781758_19563516, 4_002_19563516_29345274, 4_003_29345274_39127032, 4_004_39127032_48908790, 4_005_48908790_58690548, 4_006_58690548_68472306, 4_007_68472306_78254064, 4_008_78254064_88035822, 4_009_88035822_97817580, 4_010_97817580_107599338, 4_011_107599338_117381096, 4_012_117381096_127162854, 4_013_127162854_136944612, 4_014_136944612_146726370, 4_015_146726370_156508128, 5_000_0_9489668, 5_001_9489668_18979336, 5_002_18979336_28469004, 5_003_28469004_37958672, 5_004_37958672_47448340, 5_005_47448340_56938008, 5_006_56938008_66427676, 5_007_66427676_75917344, 5_008_75917344_85407012, 5_009_85407012_94896680, 5_010_94896680_104386348, 5_011_104386348_113876016, 5_012_113876016_123365684, 5_013_123365684_132855352, 5_014_132855352_142345020, 5_015_142345020_151834688, 6_000_0_9358535, 6_001_9358535_18717070, 6_002_18717070_28075605, 6_003_28075605_37434140, 6_004_37434140_46792675, 6_005_46792675_56151210, 6_006_56151210_65509745, 6_007_65509745_74868280, 6_008_74868280_84226815, 6_009_84226815_93585350, 6_010_93585350_102943885, 6_011_102943885_112302420, 6_012_112302420_121660955, 6_013_121660955_131019490, 6_014_131019490_140378025, 6_015_140378025_149736560, 7_000_0_9090092, 7_001_9090092_18180184, 7_002_18180184_27270276, 7_003_27270276_36360368, 7_004_36360368_45450460, 7_005_45450460_54540552, 7_006_54540552_63630644, 7_007_63630644_72720736, 7_008_72720736_81810828, 7_009_81810828_90900920, 7_010_90900920_99991012, 7_011_99991012_109081104, 7_012_109081104_118171196, 7_013_118171196_127261288, 7_014_127261288_136351380, 7_015_136351380_145441472, 8_000_0_8087576, 8_001_8087576_16175152, 8_002_16175152_24262728, 8_003_24262728_32350304, 8_004_32350304_40437880, 8_005_40437880_48525456, 8_006_48525456_56613032, 8_007_56613032_64700608, 8_008_64700608_72788184, 8_009_72788184_80875760, 8_010_80875760_88963336, 8_011_88963336_97050912, 8_012_97050912_105138488, 8_013_105138488_113226064, 8_014_113226064_121313640, 8_015_121313640_129401216, 9_000_0_7787195, 9_001_7787195_15574390, 9_002_15574390_23361585, 9_003_23361585_31148780, 9_004_31148780_38935975, 9_005_38935975_46723170, 9_006_46723170_54510365, 9_007_54510365_62297560, 9_008_62297560_70084755, 9_009_70084755_77871950, 9_010_77871950_85659145, 9_011_85659145_93446340, 9_012_93446340_101233535, 9_013_101233535_109020730, 9_014_109020730_116807925, 9_015_116807925_124595120, MT_000_0_1019, MT_001_1019_2038, MT_002_2038_3057, MT_003_3057_4076, MT_004_4076_5095, MT_005_5095_6114, MT_006_6114_7133, MT_007_7133_8152, MT_008_8152_9171, MT_009_9171_10190, MT_010_10190_11209, MT_011_11209_12228, MT_012_12228_13247, MT_013_13247_14266, MT_014_14266_15285, MT_015_15285_16304, X_000_0_10689457, X_001_10689457_21378914, X_002_21378914_32068371, X_003_32068371_42757828, X_004_42757828_53447285, X_005_53447285_64136742, X_006_64136742_74826199, X_007_74826199_85515656, X_008_85515656_96205113, X_009_96205113_106894570, X_010_106894570_117584027, X_011_117584027_128273484, X_012_128273484_138962941, X_013_138962941_149652398, X_014_149652398_160341855, X_015_160341855_171031312, Y_000_0_5734044, Y_001_5734044_11468088, Y_002_11468088_17202132, Y_003_17202132_22936176, Y_004_22936176_28670220, Y_005_28670220_34404264, Y_006_34404264_40138308, Y_007_40138308_45872352, Y_008_45872352_51606396, Y_009_51606396_57340440, Y_010_57340440_63074484, Y_011_63074484_68808528, Y_012_68808528_74542572, Y_013_74542572_80276616, Y_014_80276616_86010660, Y_015_86010660_91744704\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "directory_path = os.path.abspath(os.path.join('../src/'))\n",
    "if directory_path not in sys.path:\n",
    "    sys.path.append(directory_path)\n",
    "    \n",
    "from utils import get_edit_info_for_barcode_in_contig, get_edit_info_for_barcode_in_contig_wrapper\n",
    "\n",
    "output_folder = '/projects/ps-yeolab3/ekofman/sailor2/scripts/full_test-highmem'\n",
    "\n",
    "\n",
    "splits = [i.split(\"/\")[-1].split('_edit')[0] for i in glob('{}/edit_info/*'.format(output_folder))]\n",
    "print(\"Accessing split bams: {}\".format(', '.join(sorted(splits))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7b58ee-46a1-41e1-b3d5-d1aa4959975f",
   "metadata": {},
   "source": [
    "### Gather the edit information generated for each subcontig, and group by contig so we only have 1 edit information dataframe to process per contig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5aa1ff3-69e7-4a5c-8cee-d6af8008eec0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping edit information outputs by contig...\n",
      "\t0/352...\n",
      "\t10/352...\n",
      "\t20/352...\n",
      "\t30/352...\n",
      "\t40/352...\n",
      "\t50/352...\n",
      "\t60/352...\n",
      "\t70/352...\n",
      "\t80/352...\n",
      "\t90/352...\n",
      "\t100/352...\n",
      "\t110/352...\n",
      "\t120/352...\n",
      "\t130/352...\n",
      "\t140/352...\n",
      "\t150/352...\n",
      "\t160/352...\n",
      "\t170/352...\n",
      "\t180/352...\n",
      "\t190/352...\n",
      "\t200/352...\n",
      "\t210/352...\n",
      "\t220/352...\n",
      "\t230/352...\n",
      "\t240/352...\n",
      "\t250/352...\n",
      "\t260/352...\n",
      "\t270/352...\n",
      "\t280/352...\n",
      "\t290/352...\n",
      "\t300/352...\n",
      "\t310/352...\n",
      "\t320/352...\n",
      "\t330/352...\n",
      "\t340/352...\n",
      "\t350/352...\n",
      "Done grouping! Concatenating ...\n"
     ]
    }
   ],
   "source": [
    "all_edit_info_for_barcodes = []\n",
    "\n",
    "edit_info_grouped_per_contig = defaultdict(lambda:[])\n",
    "edit_info_grouped_per_contig_combined = defaultdict(lambda:[])\n",
    "\n",
    "num_splits = len(splits)\n",
    "print(\"Grouping edit information outputs by contig...\")\n",
    "for i, split in enumerate(splits):\n",
    "    if i%10 == 0:\n",
    "        print(\"\\t{}/{}...\".format(i, num_splits))\n",
    "    contig = split.split(\"_\")[0]\n",
    "    \n",
    "    barcode_to_coverage_dict = defaultdict()    \n",
    "    \n",
    "    barcode_to_coverage_dict = defaultdict()\n",
    "    edit_info_file = '{}/edit_info/{}_edit_info.tsv'.format(output_folder, split)\n",
    "    edit_info_df = pd.read_csv(edit_info_file, sep='\\t')\n",
    "    edit_info_df['position'] = edit_info_df['position'].astype(int)\n",
    "    edit_info_df['base_quality'] = edit_info_df['base_quality'].astype(int)\n",
    "    edit_info_df['mapping_quality'] = edit_info_df['mapping_quality'].astype(int)\n",
    "    edit_info_df['dist_from_end'] = edit_info_df['dist_from_end'].astype(int)\n",
    "\n",
    "    edit_info = pl.from_pandas(edit_info_df) \n",
    "    edit_info_grouped_per_contig[contig].append(edit_info)\n",
    "    \n",
    "    del edit_info_df\n",
    "    \n",
    "print(\"Done grouping! Concatenating ...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4b74e2a-dbc3-4b2f-82fc-bd40226f1b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done concatenating!\n"
     ]
    }
   ],
   "source": [
    "for contig, list_of_edit_info_dfs in edit_info_grouped_per_contig.items():\n",
    "    edit_info_grouped_per_contig_combined[contig] = pl.concat(list_of_edit_info_dfs)\n",
    "\n",
    "print(\"Done concatenating!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ef51b-eeba-4fb5-a716-3d6cd204c1b6",
   "metadata": {},
   "source": [
    "### Get coverage at edit positions for each contig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32c22ae8-9b11-490a-8913-9fbe3fc7ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "\n",
    "def get_coverage_for_edits_in_contig(edit_info_grouped_per_contig_combined, output_folder):\n",
    "    job_params = []\n",
    "    \n",
    "    for contig, edit_info in edit_info_grouped_per_contig_combined.items():\n",
    "        unique_barcodes = list(edit_info[\"barcode\"].unique())\n",
    "\n",
    "        for i, barcode in enumerate(unique_barcodes):                 \n",
    "            job_params.append([edit_info, contig, barcode, output_folder])  \n",
    "    return job_params\n",
    "    \n",
    "coverage_counting_job_params = get_coverage_for_edits_in_contig(edit_info_grouped_per_contig_combined, \n",
    "                                                                output_folder)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7b0cd67c-1638-4bf2-832c-acd495049c40",
   "metadata": {
    "tags": []
   },
   "source": [
    "total_params = len(coverage_counting_job_params)\n",
    "i = 0\n",
    "for edit_info, contig, barcode, output_folder in coverage_counting_job_params:\n",
    "    if i % 50 == 0:\n",
    "        print('{}/{}'.format(i, total_params))\n",
    "    get_edit_info_for_barcode_in_contig(edit_info, contig, barcode, output_folder)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851efc7-2766-4666-b426-a1ab243db920",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████████████████▊                    | 14541/32034 [09:52<20:03, 14.53it/s]"
     ]
    }
   ],
   "source": [
    "from multiprocessing import get_context\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "results = []\n",
    "with get_context(\"spawn\").Pool(processes=16) as p:\n",
    "    max_ = len(coverage_counting_job_params)\n",
    "    with tqdm(total=max_) as pbar:\n",
    "        for _ in p.imap_unordered(get_edit_info_for_barcode_in_contig_wrapper, coverage_counting_job_params):\n",
    "            pbar.update()\n",
    "            results.append(_)\n",
    "            \n",
    "total_time = time.perf_counter() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd648239-8132-4ed1-b324-09e03b198a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7caa846-b0c5-4783-b7b7-984d8f8658ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edit_info = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5647b4f-4dc5-40fc-b772-6007f8ccbfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edit_info.to_csv('{}/all_edit_info.tsv'.format(output_folder), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8620db3-658d-4e62-a352-302ce84c4c8c",
   "metadata": {},
   "source": [
    "# Group by site to get final total edit and coverage counts at each site"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03097c3-ad74-474b-8af6-921e9fc031e0",
   "metadata": {},
   "source": [
    "# Verify C>T ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803f5b24-4714-4309-a08b-9b9445218c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edit_info.groupby(['ref', 'alt']).count().plot(kind='barh', legend=False)\n",
    "plt.title(\"All edits\")\n",
    "\n",
    "base_quality_thresh = 15\n",
    "all_edit_info[all_edit_info.base_quality > base_quality_thresh].groupby(['ref', 'alt']).count().plot(kind='barh', legend=False)\n",
    "plt.title(\"Edits with base quality > {}\".format(base_quality_thresh))\n",
    "\n",
    "all_edit_info_filtered = all_edit_info[all_edit_info.base_quality > base_quality_thresh]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a97fbec-9329-4c9c-8c9b-a785a8ae376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_new_ct =  all_edit_info_filtered[(all_edit_info_filtered.ref == 'C') & (all_edit_info_filtered.alt == 'T')].sort_values('position')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b3fdc3-7fab-448f-a9e9-52cbab695e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(example_new_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be865e2-f294-4152-ae0b-85d24509a1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_new_ct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc30c7f-73aa-4be6-83a7-ff2a2072d602",
   "metadata": {},
   "source": [
    "# Cells that do have STAMP expressed versus don't...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7ee3ff-374f-4a6e-8ab7-0b1e1311c07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stamp_expression_path = \\\n",
    "'/projects/ps-yeolab3/ekofman/Sammi/MouseBrainEF1A_SingleCell_EPR_combined/\\\n",
    "4.1_cells_with_middling_stamp/stamp_expression_for_all_used_cells.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2684ed2-1146-4e4c-9b4d-186865d6e4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "stamp_expression_df = pd.read_csv(stamp_expression_path, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093153f4-7d3a-4a04-bf6d-ee855e23a02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stamp_expression_df.Stamp.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af056065-287e-4639-bc12-6f2e83468b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edit_info_filtered['edit'] = all_edit_info_filtered['ref'] + '>' + all_edit_info_filtered['alt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26edccde-cad7-48fe-ab64-20d91bcf8844",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "fractions_ct = []\n",
    "threshs = [0, 0.5, 1, 2, 3, 4, 5, 6, 6.5, 6.6]\n",
    "for thresh in threshs:\n",
    "    print(thresh)\n",
    "    barcodes_at_stamp_thresh = stamp_expression_df[stamp_expression_df.Stamp > thresh].index\n",
    "    \n",
    "    all_edit_info_filtered_in_stamp_level = all_edit_info_filtered[\n",
    "        all_edit_info_filtered.barcode.isin(barcodes_at_stamp_thresh)]\n",
    "    \n",
    "    all_edit_info_filtered_in_stamp_level.groupby(['ref', 'alt']).count().plot(kind='barh', legend=False)\n",
    "    plt.title(\"Edit Type Distribution for Cells with STAMP expression above {}\".format(thresh))\n",
    "    \n",
    "    fraction_ct = len(all_edit_info_filtered_in_stamp_level[all_edit_info_filtered_in_stamp_level['edit'] == 'C>T'])/len(all_edit_info_filtered_in_stamp_level)\n",
    "    fractions_ct.append(fraction_ct)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e6b127-1356-40c4-b700-02c31a0c5266",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(threshs, fractions_ct)\n",
    "plt.ylabel(\"Fraction of total edits that are C>T\")\n",
    "plt.xlabel(\"STAMP expression minimum\")\n",
    "plt.title(\"Enrichment for C>T edits within cells filtered by STAMP threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8800507-1d95-4bb6-94a8-9c0ec7de29fb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "fractions_ct_low = []\n",
    "threshs = [1, 2, 3, 4, 5, 6, 6.5, 6.6]\n",
    "for thresh in threshs:\n",
    "    print(thresh)\n",
    "    barcodes_at_stamp_thresh = stamp_expression_df[stamp_expression_df.Stamp < thresh].index\n",
    "    \n",
    "    all_edit_info_filtered_in_stamp_level = all_edit_info_filtered[\n",
    "        all_edit_info_filtered.barcode.isin(barcodes_at_stamp_thresh)]\n",
    "    \n",
    "    all_edit_info_filtered_in_stamp_level.groupby(['ref', 'alt']).count().plot(kind='barh', legend=False)\n",
    "    plt.title(\"Edit Type Distribution for Cells with STAMP expression below {}\".format(thresh))\n",
    "    \n",
    "    fraction_ct = len(all_edit_info_filtered_in_stamp_level[all_edit_info_filtered_in_stamp_level['edit'] == 'C>T'])/len(all_edit_info_filtered_in_stamp_level)\n",
    "    fractions_ct_low.append(fraction_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc326f-a695-41f6-bafb-5daf952a4dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(threshs, fractions_ct_low)\n",
    "plt.ylabel(\"Fraction of total edits that are C>T\")\n",
    "plt.xlabel(\"STAMP expression maximum\")\n",
    "plt.title(\"Enrichment for C>T edits within cells filtered by STAMP threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71e31ad-c7fb-4c3d-b1f2-84ed484cb525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_kernel",
   "language": "python",
   "name": "py38_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
