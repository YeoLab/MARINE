{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "834c9266-dd67-464d-b30d-eb56aee77724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam\n",
    "import os\n",
    "import sys\n",
    "from sys import getsizeof\n",
    "import time\n",
    "\n",
    "directory_path = os.path.abspath(os.path.join('../src/'))\n",
    "if directory_path not in sys.path:\n",
    "    sys.path.append(directory_path)\n",
    "    \n",
    "from read_process import get_contig_lengths_dict,\\\n",
    "incorporate_replaced_pos_info,incorporate_insertions_and_deletions,\\\n",
    "get_positions_from_md_tag,reverse_complement,get_edit_information,get_edit_information_wrapper,\\\n",
    "has_edits,get_total_coverage_for_contig_at_position,\\\n",
    "print_read_info, update_coverage_array, get_read_information, get_hamming_distance\n",
    "\n",
    "from utils import get_intervals, index_bam, write_rows_to_info_file, write_header_to_bam, \\\n",
    "write_read_to_bam_file, remove_file_if_exists, make_folder\n",
    "\n",
    "import os, psutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72010a82-87a7-4198-aa66-79f57ad70337",
   "metadata": {},
   "source": [
    "# ~~~~~~~~~~~~~~~~~~\n",
    "# Multi-processing enabled\n",
    "# ~~~~~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7166d4-3f54-4108-abb8-e978f704727a",
   "metadata": {},
   "source": [
    "# An example on a bam for 1500 cell barcodes (group0, group1, group2, group3, group4, group5, group6, group7 group8, group9, group10, group11 split from the original bam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16eb554-a261-4239-b48b-358674b9b827",
   "metadata": {},
   "source": [
    "### Should be about 1500*30,000 = 45 million reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5962296-f5be-4c0f-8323-74f591d58dea",
   "metadata": {},
   "source": [
    "#### in 10X's bam file, xf=25 means that read is uniquely mapped to a genome, and was used for counting UMI. So we should only look at reads with xf=25 from the 10X bam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fba3a6f1-60ab-4ec3-9f17-a59ab8d4bbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "bampath = '/projects/ps-yeolab3/ekofman/sailor2/data/groups_0_1_2_3_4_5_6_7_8_9_10_11_merged.bam'\n",
    "samfile = pysam.AlignmentFile(bampath, \"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "606494dd-4fd1-4760-9ad7-2c201a19e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "samfile_header = str(samfile.header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cec4616a-69eb-477c-a072-b15d570369d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19338.323"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getsizeof(samfile_header)/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ea541e-ab33-40a6-9d12-88c707c3932c",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "842901f9-7341-4c1f-87c7-0dd89cd8ac8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_edits(bampath, contig, split_index, start, end, output_folder, verbose=False):\n",
    "    time_reporting = {}\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    samfile = pysam.AlignmentFile(bampath, \"rb\")\n",
    "        \n",
    "    counts = defaultdict(lambda:defaultdict(lambda:0))\n",
    "    total_reads = 0\n",
    "    \n",
    "    bam_handles_for_barcodes = {}\n",
    "    read_lists_for_barcodes = defaultdict(lambda:[])\n",
    "    \n",
    "    reads_for_contig = samfile.fetch(contig, start, end, multiple_iterators=True)\n",
    "\n",
    "    output_file = '{}/{}_{}_{}_{}_edit_info.tsv'.format(edit_info_subfolder, contig, split_index, start, end)\n",
    "    remove_file_if_exists(output_file)\n",
    "\n",
    "    with open(output_file, 'w') as f:        \n",
    "        write_header_to_bam(f)\n",
    "\n",
    "        for i, read in enumerate(reads_for_contig):\n",
    "            total_reads += 1\n",
    "            \n",
    "            if total_reads % 1000 == 0:\n",
    "                time_reporting[total_reads] = time.perf_counter() - start_time\n",
    "\n",
    "            barcode = read.get_tag(\"CB\")\n",
    "            barcodes[contig][barcode] += 1\n",
    "\n",
    "            verbose = False\n",
    "            \n",
    "            # This is to ID the weird read with Ns\n",
    "            if 'AAACCCAAGAACTTCC-1' == barcode and split_index == '005' and contig == '17':\n",
    "                if read.query_name == 'A01535:287:H3JJHDSX7:1:2176:29188:6324':\n",
    "                    verbose = True\n",
    "            \n",
    "            error_code, list_of_rows, num_edits_of_each_type = get_read_information(read, contig, verbose=verbose)\n",
    "\n",
    "                \n",
    "            if error_code:\n",
    "                counts[contig][error_code] += 1\n",
    "            else:\n",
    "                counts[contig][EDITED_CODE] += 1\n",
    "                write_rows_to_info_file(list_of_rows, f)\n",
    "            \n",
    "            # Store each read using its string representation\n",
    "            read_as_string = read.to_string()\n",
    "            \n",
    "            if 'AAACCCAAGAACTTCC-1' == barcode and split_index == '005':\n",
    "                if read.query_name == 'A01535:287:H3JJHDSX7:1:2176:29188:6324':\n",
    "                    print(\"\\n\\n~~~ Found read, is {}\\nSplit index is {}; edit types is {}; \\n{}~~~\".format(read_as_string, \n",
    "                                                                                                     split_index,\n",
    "                                                                                                     num_edits_of_each_type,\n",
    "                                                                                                     list_of_rows\n",
    "                                                                                                    ))\n",
    "                    print(\"Read as string:\\n{}\".format(read_as_string))\n",
    "                    \n",
    "            read_lists_for_barcodes[barcode].append(read_as_string)\n",
    "            \n",
    "    \n",
    "    # Add all reads to dictionary for contig and barcode, in their string representation\n",
    "    num_barcodes = 0\n",
    "    total_bams = len(read_lists_for_barcodes)\n",
    "    \n",
    "    \n",
    "    barcode_to_concatted_reads = {}\n",
    "    for barcode, read_list in read_lists_for_barcodes.items():\n",
    "        num_barcodes += 1\n",
    "        if num_barcodes % 100 == 0:\n",
    "            #print('{}/{} processed'.format(num_barcodes, total_bams))\n",
    "            pass\n",
    "        # Concatenate the string representations of all reads for each bam-contig combination\n",
    "        all_reads_concatted = '\\n'.join(read_list)\n",
    "            \n",
    "        # Save this concatenated block of text to dictionary\n",
    "        barcode_to_concatted_reads[barcode] = all_reads_concatted\n",
    "        \n",
    "    time_reporting[total_reads] = time.perf_counter() - start_time\n",
    "    \n",
    "    samfile.close()\n",
    "    \n",
    "    return barcode_to_concatted_reads, total_reads, barcodes, counts, time_reporting\n",
    "\n",
    "\n",
    "def find_edits_and_split_bams(bampath, contig, split_index, start, end, output_folder, verbose=False):\n",
    "    barcode_to_concatted_reads, total_reads, barcodes, counts, time_reporting = find_edits(bampath, contig, split_index,\n",
    "                                                                         start, end, output_folder, verbose=verbose)    \n",
    "    return barcode_to_concatted_reads, total_reads, barcodes, counts, time_reporting\n",
    "    \n",
    "def find_edits_and_split_bams_wrapper(parameters):\n",
    "    try:\n",
    "        start_time = time.perf_counter()\n",
    "        bampath, contig, split_index, start, end, output_folder, verbose = parameters\n",
    "        label = '{}({}):{}-{}'.format(contig, split_index, start, end)\n",
    "\n",
    "        #print(\"{} ({}):{}-{}\\tfind_edits_and_split_bams\".format(contig, split_index, start, end))\n",
    "        barcode_to_concatted_reads, total_reads, barcodes, counts, time_reporting = find_edits_and_split_bams(bampath, contig, split_index, start, end, output_folder, verbose=False)\n",
    "        barcodes_df = pd.DataFrame.from_dict(barcodes)\n",
    "        counts_df = pd.DataFrame.from_dict(counts)\n",
    "        time_df = pd.DataFrame.from_dict(time_reporting, orient='index')\n",
    "        barcode_to_concatted_reads_df = pd.DataFrame.from_dict(barcode_to_concatted_reads, orient='index')\n",
    "        \n",
    "        total_time = time.perf_counter() - start_time\n",
    "        return barcode_to_concatted_reads_df, total_reads, barcodes_df, label, counts_df, time_df, total_time\n",
    "    except Exception as e:\n",
    "        print('Contig {}: {}'.format(label, e))\n",
    "        return 0, pd.DataFrame(), label, pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e80d3e0-398f-49e2-bf55-281a9621698f",
   "metadata": {},
   "source": [
    "# Go through every read and identify all edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5481bde1-a0e7-43b6-8699-1043a6d7cae3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU count: 36\n",
      "Contig 17\n",
      "16 total jobs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 9/16 [00:01<00:01,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Read ID: A01535:287:H3JJHDSX7:1:2176:29188:6324\n",
      "----------------------------\n",
      "MD tag: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 10/16 [00:02<00:01,  4.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89C11\n",
      "CIGAR string 4M102N92M90N5M\n",
      "Reference seq: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 12/16 [00:02<00:00,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAGGCCCCAAAAGTGGCGCAGCCCTCTATGGGCTCGAATTTTCTTCAGCCTCTCCAGGTCCTCACGCAGCTTGTTGTCTAGACCGTTGGCCAGAACCTGGC\n",
      "Aligned seq: AAGGCCCCAAAAGTGGCGCAGCCCTCTATGGGCTCGAATTTTCTTCAGCCTCTCCAGGTCCTCACGCAGCTTGTTGTCTAGACCGTTGGTCAGAACCTGGC\n",
      "['89', '11']\n",
      "[89, 101]\n",
      "CIGAR tuples [(0, 4), (3, 102), (0, 92), (3, 90), (0, 5)]\n",
      "AAGGNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNCCCCAAAAGTGGCGCAGCCCTCTATGGGCTCGAATTTTCTTCAGCCTCTCCAGGTCCTCACGCAGCTTGTTGTCTAGACCGTTGGTCAGAACNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNCTGGC\n",
      "aaggccccaaaagtggcgcagccctctatgggctcgaattttcttcagcctctccaggtcctcacgcagcttgttgtctagaccgttggCcagaacctggc\n",
      "aaggnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnNnnnnnnnnnnnNnnnnccccaaaagtggcgcagccctctatgggctcgaattttcttcagcctctccaggtcctcacgcagcttgttgtctagaccgttggtcagaacnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnctggc\n",
      "alt bases ['N', 'N']\n",
      "ref bases ['C']\n",
      "\n",
      "\n",
      "~~~ Found read, is A01535:287:H3JJHDSX7:1:2176:29188:6324\t16\t17\t33952115\t255\t4M102N92M90N5M\t*\t0\t0\tAAGGCCCCAAAAGTGGCGCAGCCCTCTATGGGCTCGAATTTTCTTCAGCCTCTCCAGGTCCTCACGCAGCTTGTTGTCTAGACCGTTGGTCAGAACCTGGC\t:FFFFFFF:FFFFFFFFF,FFFFFFFF,FFFF,FFF::FF:FF:FFFFFFFFFFFFFFFFFFF,FFFFFFFFF,FFFFFFFFFFFFFFFFFFFFFFFFFFF\tNH:i:1\tHI:i:1\tAS:i:101\tnM:i:1\tTX:Z:ENSMUST00000008812,+383,101M;ENSMUST00000172799,+935,101M;ENSMUST00000174745,+319,101M;ENSMUST00000174758,+282,101M\tGX:Z:ENSMUSG00000008668\tGN:Z:Rps18\tfx:Z:ENSMUSG00000008668\tRE:A:E\txf:i:25\tCR:Z:AAACCCAAGAACTTCC\tCY:Z:FF:F,FFFFFFFF:FF\tCB:Z:AAACCCAAGAACTTCC-1\tUR:Z:CGCCCCTGCGCA\tUY:Z:FFFFFFFF:,FF\tUB:Z:CGCCCCTGCTCA\tNM:i:1\tMD:Z:89C11\tRG:Z:ms_hippo_stamp_EIF4A_batch2:0:1:H3JJHDSX7:1-32F01FC\n",
      "Split index is 005; edit types is defaultdict(<function get_read_information.<locals>.<lambda> at 0x2af93adbd4d0>, {'G>N': 1}); \n",
      "[['AAACCCAAGAACTTCC-1', '17', '33952203', 'G', 'N', 'A01535:287:H3JJHDSX7:1:2176:29188:6324', '-', '89', '37', '255']]~~~\n",
      "Read as string:\n",
      "A01535:287:H3JJHDSX7:1:2176:29188:6324\t16\t17\t33952115\t255\t4M102N92M90N5M\t*\t0\t0\tAAGGCCCCAAAAGTGGCGCAGCCCTCTATGGGCTCGAATTTTCTTCAGCCTCTCCAGGTCCTCACGCAGCTTGTTGTCTAGACCGTTGGTCAGAACCTGGC\t:FFFFFFF:FFFFFFFFF,FFFFFFFF,FFFF,FFF::FF:FF:FFFFFFFFFFFFFFFFFFF,FFFFFFFFF,FFFFFFFFFFFFFFFFFFFFFFFFFFF\tNH:i:1\tHI:i:1\tAS:i:101\tnM:i:1\tTX:Z:ENSMUST00000008812,+383,101M;ENSMUST00000172799,+935,101M;ENSMUST00000174745,+319,101M;ENSMUST00000174758,+282,101M\tGX:Z:ENSMUSG00000008668\tGN:Z:Rps18\tfx:Z:ENSMUSG00000008668\tRE:A:E\txf:i:25\tCR:Z:AAACCCAAGAACTTCC\tCY:Z:FF:F,FFFFFFFF:FF\tCB:Z:AAACCCAAGAACTTCC-1\tUR:Z:CGCCCCTGCGCA\tUY:Z:FFFFFFFF:,FF\tUB:Z:CGCCCCTGCTCA\tNM:i:1\tMD:Z:89C11\tRG:Z:ms_hippo_stamp_EIF4A_batch2:0:1:H3JJHDSX7:1-32F01FC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:20<00:00,  1.27s/it]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "print(\"CPU count: {}\".format(multiprocessing.cpu_count()))\n",
    "\n",
    "output_folder = '/projects/ps-yeolab3/ekofman/sailor2/scripts/check_against_pileup_all_cells_threaded_outs_bigger'\n",
    "\n",
    "contig_lengths_dict = get_contig_lengths_dict(samfile)\n",
    "\n",
    "# Print info?\n",
    "verbose = False \n",
    "EDITED_CODE = 'edited'\n",
    "\n",
    "# How many subcontigs to split each contig into to leverage multi-processing\n",
    "num_intervals = 16\n",
    "\n",
    "num_reads_to_coverage_dict_kb = {}\n",
    "num_reads_to_seconds = {}\n",
    "\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "total_seconds_for_reads = {0: 1}\n",
    "\n",
    "barcodes = defaultdict(lambda:defaultdict(lambda:0))\n",
    "\n",
    "jobs = []\n",
    "for contig in contig_lengths_dict.keys():\n",
    "    # Skip useless contigs\n",
    "    if len(contig) > 5 or contig == 'Stamp' or contig != '17':\n",
    "        continue\n",
    "        \n",
    "    print(\"Contig {}\".format(contig))\n",
    "    contig_length = contig_lengths_dict.get(contig)\n",
    "    intervals_for_contig = get_intervals(contig, contig_lengths_dict, num_intervals)\n",
    "    \n",
    "    # Make subfolder in which to information about edits\n",
    "    edit_info_subfolder = '{}/edit_info'.format(output_folder)\n",
    "    make_folder(edit_info_subfolder)\n",
    "        \n",
    "    # Set up for pool\n",
    "    for split_index, interval in enumerate(intervals_for_contig):\n",
    "        split_index = str(split_index).zfill(3)\n",
    "        parameters = [bampath, contig, split_index, interval[0], interval[1], output_folder, verbose]\n",
    "        jobs.append(parameters)\n",
    "    \n",
    "print(\"{} total jobs\".format(len(jobs)))\n",
    "\n",
    "# Pooling\n",
    "results = []\n",
    "overall_total_reads = 0\n",
    "with Pool(processes=16) as p:\n",
    "    max_ = len(jobs)\n",
    "    with tqdm(total=max_) as pbar:\n",
    "        for _ in p.imap_unordered(find_edits_and_split_bams_wrapper, jobs):\n",
    "            pbar.update()\n",
    "            results.append(_)\n",
    "            \n",
    "            total_reads = _[1]\n",
    "            total_time = time.perf_counter() - start_time\n",
    "\n",
    "            overall_total_reads += total_reads\n",
    "            total_seconds_for_reads[overall_total_reads] = total_time\n",
    "\n",
    "overall_time = time.perf_counter() - start_time \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "997092c0-977e-4b1c-9399-b762915cde70",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_as_string = 'A01535:287:H3JJHDSX7:1:2176:29188:6324\t16\t17\t33952115\t255\t4M102N92M90N5M\t*\t0\t0\tAAGGCCCCAAAAGTGGCGCAGCCCTCTATGGGCTCGAATTTTCTTCAGCCTCTCCAGGTCCTCACGCAGCTTGTTGTCTAGACCGTTGGTCAGAACCTGGC\t:FFFFFFF:FFFFFFFFF,FFFFFFFF,FFFF,FFF::FF:FF:FFFFFFFFFFFFFFFFFFF,FFFFFFFFF,FFFFFFFFFFFFFFFFFFFFFFFFFFF\tNH:i:1\tHI:i:1\tAS:i:101\tnM:i:1\tTX:Z:ENSMUST00000008812,+383,101M;ENSMUST00000172799,+935,101M;ENSMUST00000174745,+319,101M;ENSMUST00000174758,+282,101M\tGX:Z:ENSMUSG00000008668\tGN:Z:Rps18\tfx:Z:ENSMUSG00000008668\tRE:A:E\txf:i:25\tCR:Z:AAACCCAAGAACTTCC\tCY:Z:FF:F,FFFFFFFF:FF\tCB:Z:AAACCCAAGAACTTCC-1\tUR:Z:CGCCCCTGCGCA\tUY:Z:FFFFFFFF:,FF\tUB:Z:CGCCCCTGCTCA\tNM:i:1\tMD:Z:89C11\tRG:Z:ms_hippo_stamp_EIF4A_batch2:0:1:H3JJHDSX7:1-32F01FC'\n",
    "read = pysam.AlignedSegment.fromstring(read_as_string, samfile.header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea8ff9b2-29b9-4ca3-b36b-c1de2388ee18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaggccccaaaagtggcgcagccctctatgggctcgaattttcttcagcctctccaggtcctcacgcagcttgttgtctagaccgttggTcagaacctggc\n",
      "\n",
      "\n",
      "aaggNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNccccaaaagtggcgcagccctctatgggctcgaattttcttcagcctctccaggtcctcacgcagcttgttgtctagaccgttggTcagaacNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNctggc\n"
     ]
    }
   ],
   "source": [
    "reverse = False\n",
    "md_tag = read.get_tag('MD')\n",
    "cigarstring = read.cigarstring\n",
    "\n",
    "cigar_tuples = read.cigartuples\n",
    "aligned_seq = read.get_forward_sequence()\n",
    "query_qualities = read.query_qualities\n",
    "\n",
    "if not reverse:\n",
    "    aligned_seq = reverse_complement(aligned_seq)\n",
    "\n",
    "reference_seq = read.get_reference_sequence().lower()\n",
    "\n",
    "if verbose:\n",
    "    print(\"MD tag:\", md_tag)\n",
    "    print(\"CIGAR string\", cigarstring)\n",
    "    print(\"Reference seq:\", reference_seq.upper())\n",
    "    print(\"Aligned seq:\", aligned_seq)\n",
    "\n",
    "\n",
    "indicated_aligned_seq, alt_bases = incorporate_replaced_pos_info(aligned_seq, positions_replaced)\n",
    "print(indicated_aligned_seq)\n",
    "fixed_aligned_seq = incorporate_insertions_and_deletions(indicated_aligned_seq, cigar_tuples)\n",
    "print('\\n')\n",
    "print(fixed_aligned_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a13fe663-ba73-4544-9bf2-a77055e9e4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaggccccaaaagtggcgcagccctctatgggctcgaattttcttcagcctctccaggtcctcacgcagcttgttgtctagaccgttggCcagaacctggc\n",
      "\n",
      "aaggNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNccccaaaagtggcgcagccctctatgggctcgaattttcttcagcctctccaggtcctcacgcagcttgttgtctagaccgttggCcagaacNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNctggc\n"
     ]
    }
   ],
   "source": [
    "\n",
    "indicated_reference_seq, alt_bases = incorporate_replaced_pos_info(reference_seq, positions_replaced)\n",
    "print(indicated_reference_seq)\n",
    "fixed_reference_seq = incorporate_insertions_and_deletions(indicated_reference_seq, cigar_tuples)\n",
    "print()\n",
    "print(fixed_reference_seq)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b52c0d8-44c1-40db-b65f-b1553f8fa441",
   "metadata": {},
   "source": [
    "fixed_reference_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6e7d645-76f5-48b6-a788-cd1c9204e244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aaggNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNccccaaaagtggcgcagccctctatgggctcgaattttcttcagcctctccaggtcctcacgcagcttgttgtctagaccgttggTcagaacNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNctggc'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_aligned_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63ea68fa-c5ca-4a11-a5dd-3d92463c95cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AAGGNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNCCCCAAAAGTGGCGCAGCCCTCTATGGGCTCGAATTTTCTTCAGCCTCTCCAGGTCCTCACGCAGCTTGTTGTCTAGACCGTTGGTCAGAACNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNCTGGC'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_aligned_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97bf427b-e944-443f-af77-2225085e3a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[89, 101]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions_replaced = get_positions_from_md_tag(md_tag, verbose=verbose)\n",
    "positions_replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d653aa1a-b525-4132-9794-4dea3591c6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aaggnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnNnnnnnnnnnnnNnnnnccccaaaagtggcgcagccctctatgggctcgaattttcttcagcctctccaggtcctcacgcagcttgttgtctagaccgttggtcagaacnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnctggc'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions_replaced = get_positions_from_md_tag(md_tag, verbose=verbose)\n",
    "\n",
    "\n",
    "indicated_aligned_seq, alt_bases = incorporate_replaced_pos_info(fixed_aligned_seq, positions_replaced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a2fc8ef-f21e-46bf-b8b2-39fa0adefb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaggccccaaaagtggcgcagccctctatgggctcgaattttcttcagcctctccaggtcctcacgcagcttgttgtctagaccgttggTcagaacctggc\n",
      "aaggccccaaaagtggcgcagccctctatgggctcgaattttcttcagcctctccaggtcctcacgcagcttgttgtctagaccgttggccagaacctggc\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df0f1977-a6e9-4653-97ef-fb2afa07ad19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 26.106521040201187 seconds\n",
      "Total time: 0.4351086840033531 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"Total time: {} seconds\".format(overall_time))\n",
    "print(\"Total time: {} minutes\".format(overall_time/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7ad726-4045-4229-ae90-369ae6327f06",
   "metadata": {},
   "source": [
    "# More helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "139466f8-428d-418b-808b-103f70897871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_bam(bam_file_name):\n",
    "    output_name = bam_file_name.split(\"bam\")[0] + \".sorted.bam\"\n",
    "    pysam.sort(\"-o\", output_name, bam_file_name)  \n",
    "    return output_name\n",
    "\n",
    "def write_reads_to_file(reads, bam_file_name, header_string):\n",
    "    with pysam.AlignmentFile(bam_file_name, \"wb\", text=header_string) as bam_handle:\n",
    "        for read_str in reads:\n",
    "            read = pysam.AlignedSegment.fromstring(read_str, bam_handle.header)\n",
    "            bam_handle.write(read) \n",
    "    bam_handle.close()\n",
    "            \n",
    "def write_reads_to_file_wrapper(parameters):\n",
    "    reads, bam_file_name, header_string = parameters\n",
    "    write_reads_to_file(reads, bam_file_name, header_string)\n",
    "    \n",
    "    try:\n",
    "        index_bam(bam_file_name)\n",
    "    except Exception as e:\n",
    "        print(\"Failed at indexing {}\".format(bam_file_name))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75188cf5-e1f2-4390-ba25-541e0f7b2800",
   "metadata": {},
   "source": [
    "# Combine all of the reads (string representation) for each barcode\n",
    "## Groups the results from each sub-contig segment above, for example the reads from the first half of chr1 and those from the second half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7830a72a-38bb-4e9a-ab7a-fe3867daf56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length mismatch: Expected axis has 0 elements, new values have 1 elements MT(000):0-1019\n",
      "Length mismatch: Expected axis has 0 elements, new values have 1 elements MT(001):1019-2038\n",
      "Length mismatch: Expected axis has 0 elements, new values have 1 elements Y(004):22936176-28670220\n",
      "Length mismatch: Expected axis has 0 elements, new values have 1 elements Y(005):28670220-34404264\n",
      "Length mismatch: Expected axis has 0 elements, new values have 1 elements Y(008):45872352-51606396\n",
      "Length mismatch: Expected axis has 0 elements, new values have 1 elements Y(009):51606396-57340440\n",
      "Length mismatch: Expected axis has 0 elements, new values have 1 elements Y(012):68808528-74542572\n",
      "Length mismatch: Expected axis has 0 elements, new values have 1 elements Y(014):80276616-86010660\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy \n",
    "\n",
    "results_copy = deepcopy(results)\n",
    "\n",
    "num_barcodes_to_time = {}\n",
    "num_contigs_to_time = {}\n",
    "\n",
    "overall_label_to_list_of_contents = defaultdict(lambda:{})\n",
    "\n",
    "for barcode_to_concatted_reads_df, total_reads, barcodes_df, label, counts_df, time_df, total_time in results_copy:\n",
    "    try:\n",
    "        barcode_to_concatted_reads_df.columns = ['contents']\n",
    "        barcode_to_concatted_reads_df['barcode'] = [b.split('/')[-1].split('.bam')[0] for b in barcode_to_concatted_reads_df.index]\n",
    "        contig = label.split('(')[0]\n",
    "        barcode_to_concatted_reads_df['barcode_contig'] = barcode_to_concatted_reads_df['barcode'] + '_' + contig\n",
    "        overall_label_to_list_of_contents[contig][label] = barcode_to_concatted_reads_df\n",
    "    except Exception as e:\n",
    "        print(e, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de955557-09d6-4d68-b415-8e16802c64ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall contigs:\n",
      "\n",
      "\t dict_keys(['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '3', '4', '5', '6', '7', '8', '9', 'MT', 'X', 'Y'])\n",
      "\n",
      "Subcontig regions for an example contig (17):\n",
      "\n",
      "\t ['17(000):0-5936705', '17(001):5936705-11873410', '17(002):11873410-17810115', '17(003):17810115-23746820', '17(004):23746820-29683525', '17(005):29683525-35620230', '17(006):35620230-41556935', '17(007):41556935-47493640', '17(008):47493640-53430345', '17(009):53430345-59367050', '17(010):59367050-65303755', '17(011):65303755-71240460', '17(012):71240460-77177165', '17(013):77177165-83113870', '17(014):83113870-89050575', '17(015):89050575-94987280']\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall contigs:\\n\\n\\t\",overall_label_to_list_of_contents.keys())\n",
    "print(\"\\nSubcontig regions for an example contig (17):\\n\\n\\t\",sorted(overall_label_to_list_of_contents.get('17').keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8134fb0e-6dd2-4348-9848-ee1cf666cd3b",
   "metadata": {},
   "source": [
    "### Generate list of jobs to be multiprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6589a91c-43b3-44eb-9a06-ae430dba3c3b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contig: 1\n",
      "0/1500 barcodes\n",
      "100/1500 barcodes\n",
      "200/1500 barcodes\n",
      "300/1500 barcodes\n",
      "400/1500 barcodes\n",
      "500/1500 barcodes\n",
      "600/1500 barcodes\n",
      "700/1500 barcodes\n",
      "800/1500 barcodes\n",
      "900/1500 barcodes\n",
      "1000/1500 barcodes\n",
      "1100/1500 barcodes\n",
      "1200/1500 barcodes\n",
      "1300/1500 barcodes\n",
      "1400/1500 barcodes\n",
      "Contig: 10\n",
      "0/1500 barcodes\n",
      "100/1500 barcodes\n",
      "200/1500 barcodes\n",
      "300/1500 barcodes\n",
      "400/1500 barcodes\n",
      "500/1500 barcodes\n",
      "600/1500 barcodes\n",
      "700/1500 barcodes\n",
      "800/1500 barcodes\n",
      "900/1500 barcodes\n",
      "1000/1500 barcodes\n",
      "1100/1500 barcodes\n",
      "1200/1500 barcodes\n",
      "1300/1500 barcodes\n",
      "1400/1500 barcodes\n",
      "Contig: 11\n",
      "0/1500 barcodes\n",
      "100/1500 barcodes\n",
      "200/1500 barcodes\n",
      "300/1500 barcodes\n",
      "400/1500 barcodes\n",
      "500/1500 barcodes\n",
      "600/1500 barcodes\n",
      "700/1500 barcodes\n",
      "800/1500 barcodes\n",
      "900/1500 barcodes\n",
      "1000/1500 barcodes\n",
      "1100/1500 barcodes\n",
      "1200/1500 barcodes\n",
      "1300/1500 barcodes\n",
      "1400/1500 barcodes\n",
      "Contig: 12\n",
      "0/1500 barcodes\n",
      "100/1500 barcodes\n",
      "200/1500 barcodes\n",
      "300/1500 barcodes\n",
      "400/1500 barcodes\n",
      "500/1500 barcodes\n",
      "600/1500 barcodes\n",
      "700/1500 barcodes\n",
      "800/1500 barcodes\n",
      "900/1500 barcodes\n",
      "1000/1500 barcodes\n",
      "1100/1500 barcodes\n",
      "1200/1500 barcodes\n",
      "1300/1500 barcodes\n",
      "1400/1500 barcodes\n",
      "Contig: 13\n",
      "0/1500 barcodes\n",
      "100/1500 barcodes\n",
      "200/1500 barcodes\n",
      "300/1500 barcodes\n",
      "400/1500 barcodes\n",
      "500/1500 barcodes\n",
      "600/1500 barcodes\n",
      "700/1500 barcodes\n",
      "800/1500 barcodes\n",
      "900/1500 barcodes\n",
      "1000/1500 barcodes\n",
      "1100/1500 barcodes\n",
      "1200/1500 barcodes\n",
      "1300/1500 barcodes\n",
      "1400/1500 barcodes\n",
      "Contig: 14\n",
      "0/1500 barcodes\n",
      "100/1500 barcodes\n",
      "200/1500 barcodes\n",
      "300/1500 barcodes\n",
      "400/1500 barcodes\n",
      "500/1500 barcodes\n",
      "600/1500 barcodes\n",
      "700/1500 barcodes\n",
      "800/1500 barcodes\n",
      "900/1500 barcodes\n",
      "1000/1500 barcodes\n",
      "1100/1500 barcodes\n",
      "1200/1500 barcodes\n",
      "1300/1500 barcodes\n",
      "1400/1500 barcodes\n",
      "Contig: 15\n",
      "0/1500 barcodes\n",
      "100/1500 barcodes\n",
      "200/1500 barcodes\n",
      "300/1500 barcodes\n",
      "400/1500 barcodes\n",
      "500/1500 barcodes\n",
      "600/1500 barcodes\n",
      "700/1500 barcodes\n",
      "800/1500 barcodes\n",
      "900/1500 barcodes\n",
      "1000/1500 barcodes\n",
      "1100/1500 barcodes\n",
      "1200/1500 barcodes\n",
      "1300/1500 barcodes\n",
      "1400/1500 barcodes\n",
      "Contig: 16\n",
      "0/1500 barcodes\n",
      "100/1500 barcodes\n",
      "200/1500 barcodes\n",
      "300/1500 barcodes\n",
      "400/1500 barcodes\n",
      "500/1500 barcodes\n",
      "600/1500 barcodes\n",
      "700/1500 barcodes\n",
      "800/1500 barcodes\n",
      "900/1500 barcodes\n",
      "1000/1500 barcodes\n",
      "1100/1500 barcodes\n",
      "1200/1500 barcodes\n",
      "1300/1500 barcodes\n",
      "1400/1500 barcodes\n",
      "Contig: 17\n",
      "0/1500 barcodes\n",
      "100/1500 barcodes\n",
      "200/1500 barcodes\n",
      "300/1500 barcodes\n",
      "400/1500 barcodes\n",
      "500/1500 barcodes\n",
      "600/1500 barcodes\n",
      "700/1500 barcodes\n",
      "800/1500 barcodes\n",
      "900/1500 barcodes\n",
      "1000/1500 barcodes\n",
      "1100/1500 barcodes\n",
      "1200/1500 barcodes\n",
      "1300/1500 barcodes\n",
      "1400/1500 barcodes\n",
      "Contig: 18\n",
      "0/1500 barcodes\n",
      "100/1500 barcodes\n",
      "200/1500 barcodes\n",
      "300/1500 barcodes\n",
      "400/1500 barcodes\n",
      "500/1500 barcodes\n",
      "600/1500 barcodes\n",
      "700/1500 barcodes\n",
      "800/1500 barcodes\n",
      "900/1500 barcodes\n",
      "1000/1500 barcodes\n",
      "1100/1500 barcodes\n",
      "1200/1500 barcodes\n",
      "1300/1500 barcodes\n",
      "1400/1500 barcodes\n",
      "Contig: 19\n",
      "0/1500 barcodes\n",
      "100/1500 barcodes\n",
      "200/1500 barcodes\n",
      "300/1500 barcodes\n",
      "400/1500 barcodes\n",
      "500/1500 barcodes\n",
      "600/1500 barcodes\n",
      "700/1500 barcodes\n",
      "800/1500 barcodes\n",
      "900/1500 barcodes\n",
      "1000/1500 barcodes\n",
      "1100/1500 barcodes\n",
      "1200/1500 barcodes\n",
      "1300/1500 barcodes\n",
      "1400/1500 barcodes\n",
      "Contig: 2\n",
      "0/1500 barcodes\n",
      "100/1500 barcodes\n",
      "200/1500 barcodes\n",
      "300/1500 barcodes\n",
      "400/1500 barcodes\n",
      "500/1500 barcodes\n",
      "600/1500 barcodes\n",
      "700/1500 barcodes\n",
      "800/1500 barcodes\n",
      "900/1500 barcodes\n",
      "1000/1500 barcodes\n",
      "1100/1500 barcodes\n",
      "1200/1500 barcodes\n",
      "1300/1500 barcodes\n",
      "1400/1500 barcodes\n",
      "Contig: 3\n",
      "0/1500 barcodes\n",
      "100/1500 barcodes\n",
      "200/1500 barcodes\n",
      "300/1500 barcodes\n",
      "400/1500 barcodes\n",
      "500/1500 barcodes\n",
      "600/1500 barcodes\n",
      "700/1500 barcodes\n",
      "800/1500 barcodes\n",
      "900/1500 barcodes\n",
      "1000/1500 barcodes\n",
      "1100/1500 barcodes\n",
      "1200/1500 barcodes\n",
      "1300/1500 barcodes\n",
      "1400/1500 barcodes\n",
      "Contig: 4\n",
      "0/1500 barcodes\n",
      "100/1500 barcodes\n",
      "200/1500 barcodes\n",
      "300/1500 barcodes\n",
      "400/1500 barcodes\n",
      "500/1500 barcodes\n",
      "600/1500 barcodes\n",
      "700/1500 barcodes\n",
      "800/1500 barcodes\n",
      "900/1500 barcodes\n",
      "1000/1500 barcodes\n",
      "1100/1500 barcodes\n",
      "1200/1500 barcodes\n",
      "1300/1500 barcodes\n",
      "1400/1500 barcodes\n",
      "Contig: 5\n",
      "0/1500 barcodes\n",
      "100/1500 barcodes\n",
      "200/1500 barcodes\n",
      "300/1500 barcodes\n",
      "400/1500 barcodes\n",
      "500/1500 barcodes\n",
      "600/1500 barcodes\n",
      "700/1500 barcodes\n",
      "800/1500 barcodes\n",
      "900/1500 barcodes\n",
      "1000/1500 barcodes\n",
      "1100/1500 barcodes\n",
      "1200/1500 barcodes\n",
      "1300/1500 barcodes\n",
      "1400/1500 barcodes\n",
      "Contig: 6\n",
      "0/1500 barcodes\n",
      "100/1500 barcodes\n",
      "200/1500 barcodes\n",
      "300/1500 barcodes\n",
      "400/1500 barcodes\n",
      "500/1500 barcodes\n",
      "600/1500 barcodes\n",
      "700/1500 barcodes\n",
      "800/1500 barcodes\n",
      "900/1500 barcodes\n",
      "1000/1500 barcodes\n",
      "1100/1500 barcodes\n",
      "1200/1500 barcodes\n",
      "1300/1500 barcodes\n",
      "1400/1500 barcodes\n",
      "Contig: 7\n",
      "0/1500 barcodes\n",
      "100/1500 barcodes\n",
      "200/1500 barcodes\n",
      "300/1500 barcodes\n",
      "400/1500 barcodes\n",
      "500/1500 barcodes\n",
      "600/1500 barcodes\n",
      "700/1500 barcodes\n",
      "800/1500 barcodes\n",
      "900/1500 barcodes\n",
      "1000/1500 barcodes\n",
      "1100/1500 barcodes\n",
      "1200/1500 barcodes\n",
      "1300/1500 barcodes\n",
      "1400/1500 barcodes\n",
      "Contig: 8\n",
      "0/1500 barcodes\n",
      "100/1500 barcodes\n",
      "200/1500 barcodes\n",
      "300/1500 barcodes\n",
      "400/1500 barcodes\n",
      "500/1500 barcodes\n",
      "600/1500 barcodes\n",
      "700/1500 barcodes\n",
      "800/1500 barcodes\n",
      "900/1500 barcodes\n",
      "1000/1500 barcodes\n",
      "1100/1500 barcodes\n",
      "1200/1500 barcodes\n",
      "1300/1500 barcodes\n",
      "1400/1500 barcodes\n",
      "Contig: 9\n",
      "0/1500 barcodes\n",
      "100/1500 barcodes\n",
      "200/1500 barcodes\n",
      "300/1500 barcodes\n",
      "400/1500 barcodes\n",
      "500/1500 barcodes\n",
      "600/1500 barcodes\n",
      "700/1500 barcodes\n",
      "800/1500 barcodes\n",
      "900/1500 barcodes\n",
      "1000/1500 barcodes\n",
      "1100/1500 barcodes\n",
      "1200/1500 barcodes\n",
      "1300/1500 barcodes\n",
      "1400/1500 barcodes\n",
      "Contig: MT\n",
      "0/1499 barcodes\n",
      "100/1499 barcodes\n",
      "200/1499 barcodes\n",
      "300/1499 barcodes\n",
      "400/1499 barcodes\n",
      "500/1499 barcodes\n",
      "600/1499 barcodes\n",
      "700/1499 barcodes\n",
      "800/1499 barcodes\n",
      "900/1499 barcodes\n",
      "1000/1499 barcodes\n",
      "1100/1499 barcodes\n",
      "1200/1499 barcodes\n",
      "1300/1499 barcodes\n",
      "1400/1499 barcodes\n",
      "Contig: X\n",
      "0/1500 barcodes\n",
      "100/1500 barcodes\n",
      "200/1500 barcodes\n",
      "300/1500 barcodes\n",
      "400/1500 barcodes\n",
      "500/1500 barcodes\n",
      "600/1500 barcodes\n",
      "700/1500 barcodes\n",
      "800/1500 barcodes\n",
      "900/1500 barcodes\n",
      "1000/1500 barcodes\n",
      "1100/1500 barcodes\n",
      "1200/1500 barcodes\n",
      "1300/1500 barcodes\n",
      "1400/1500 barcodes\n",
      "Contig: Y\n",
      "0/1063 barcodes\n",
      "100/1063 barcodes\n",
      "200/1063 barcodes\n",
      "300/1063 barcodes\n",
      "400/1063 barcodes\n",
      "500/1063 barcodes\n",
      "600/1063 barcodes\n",
      "700/1063 barcodes\n",
      "800/1063 barcodes\n",
      "900/1063 barcodes\n",
      "1000/1063 barcodes\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "start_time = time.perf_counter() \n",
    "\n",
    "# Get the bam header, which will be used for each of the split bams too\n",
    "header_string = str(samfile.header)\n",
    "\n",
    "# Make a subfolder into which the split bams will be placed\n",
    "split_bams_folder = '{}/split_bams'.format(output_folder)\n",
    "if not os.path.exists(split_bams_folder):\n",
    "    os.mkdir(split_bams_folder)\n",
    "\n",
    "    \n",
    "num_contigs = 0\n",
    "jobs_params = []\n",
    "\n",
    "for contig, df_dict in overall_label_to_list_of_contents.items():\n",
    "    num_contigs += 1\n",
    "    print(\"Contig: {}\".format(contig))\n",
    "    \n",
    "    # Make a sub-subfolder to put the bams for this specific contig\n",
    "    contig_folder = '{}/{}/'.format(split_bams_folder, contig)\n",
    "    if not os.path.exists(contig_folder):\n",
    "        os.mkdir(contig_folder)\n",
    "    \n",
    "    # Sort the subcontig regions such that the reads are properly ordered \n",
    "    sorted_subcontig_names = sorted(df_dict.keys())\n",
    "    sorted_subcontig_dfs = []\n",
    "    for n in sorted_subcontig_names:\n",
    "        sorted_subcontig_dfs.append(df_dict.get(n))\n",
    "        \n",
    "    # All of the reads for all of the barcodes are in this dataframe\n",
    "    all_contents_df = pd.concat(sorted_subcontig_dfs)\n",
    "    \n",
    "    # Get all the unique barcodes\n",
    "    all_barcodes = list(all_contents_df.barcode.unique())\n",
    "    \n",
    "    \n",
    "    for i, barcode in enumerate(all_barcodes):            \n",
    "        if i % 100 == 0:\n",
    "            print('{}/{} barcodes'.format(i, len(all_barcodes)))\n",
    "\n",
    "        # Combine the reads (in string representation) for all rows corresponding to a barcode\n",
    "        contents_for_barcode = all_contents_df[all_contents_df.barcode == barcode]\n",
    "                \n",
    "        all_contents_text = '\\n'.join(contents_for_barcode.contents)\n",
    "        \n",
    "        # Turn the newline-delimited block of text back into list of reads as strings\n",
    "        reads = all_contents_text.split('\\n')\n",
    "            \n",
    "        # Remove duplicates\n",
    "        reads_deduped = list(OrderedDict.fromkeys(reads))\n",
    "                \n",
    "        # Establish the name of the split bam that will be generated\n",
    "        bam_file_name = '{}/{}_{}.bam'.format(contig_folder, contig, barcode)\n",
    "        \n",
    "        # Add parameters to list of jobs\n",
    "        jobs_params.append([reads_deduped, bam_file_name, header_string])\n",
    "        \n",
    "            \n",
    "total_time = time.perf_counter() - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d100d6c4-3e3f-46a8-8bdf-6f6e45bad65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to prepare list for multiprocess-writing bams: 25 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"Total time to prepare list for multiprocess-writing bams: {} minutes\".format(round(total_time/60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd90c81-f322-46fb-965d-186f9f53630d",
   "metadata": {},
   "source": [
    "# Generate bams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d728e6e-bb60-4996-a35e-346c8e62d867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2268/32562 [00:33<06:59, 72.25it/s]"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "with Pool(processes=16) as p:\n",
    "    max_ = len(jobs_params)\n",
    "    with tqdm(total=max_) as pbar:\n",
    "        for _ in p.imap_unordered(write_reads_to_file_wrapper, jobs_params):\n",
    "            pbar.update()\n",
    "\n",
    "total_time = time.perf_counter() - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0577cf-6600-4107-9db0-8399417d4c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total time to write bams: {} minutes\".format(round(total_time/60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa2d81-c8bc-4bf9-9487-b67bea3ec0eb",
   "metadata": {},
   "source": [
    "# Time profiling of the edit-counting step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84dae60-ee51-4358-9784-596af7449002",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_contig_times = {}\n",
    "all_read_info_dfs = []\n",
    "all_time_dfs = []\n",
    "\n",
    "total_times = {}\n",
    "for result in results:\n",
    "    \n",
    "    label = result[3]\n",
    "\n",
    "    try:\n",
    "        total_time = result[6]\n",
    "        total_times[label] = total_time\n",
    "        \n",
    "        time_df = result[5]\n",
    "        all_time_dfs.append(time_df)\n",
    "        total_time_for_contig = (float(time_df.max()))\n",
    "        total_contig_times[label] = total_time_for_contig\n",
    "        \n",
    "        read_info_df = result[4]\n",
    "        read_info_df.columns = [label]\n",
    "        all_read_info_dfs.append(read_info_df)\n",
    "    except Exception as e:\n",
    "        print(e, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8708d9-72d1-4cd3-ae8c-8b9622e522b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_contig_times_df = pd.DataFrame.from_dict(total_contig_times, orient='index', columns=['seconds']).sort_values('seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72adfbd7-f62c-4eaf-ba72-3cb457e69515",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Total time without threading: {} minutes'.format(round(total_contig_times_df.seconds.sum()/60, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ff6910-f0d7-46dc-8014-7d9301763a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reads_df = pd.concat(all_read_info_dfs,axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88823e6-5ce4-4c90-b5ba-28ef2c245857",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reads_and_times_df = total_reads_df.join(total_contig_times_df)\n",
    "\n",
    "plt.scatter(total_reads_and_times_df.edited, total_reads_and_times_df.seconds, s=1)\n",
    "plt.scatter(total_reads_and_times_df.no_edits, total_reads_and_times_df.seconds, s=1)\n",
    "\n",
    "plt.title(\"Total processing time vs number of reads\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.xlabel(\"Reads\")\n",
    "plt.legend(['Reads with edits', 'Read without edits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202d2ff9-8a63-49e0-ab56-678d89cebb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(total_seconds_for_reads, orient='index').sort_index().plot(legend=False)\n",
    "plt.xlabel(\"Reads\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.title(\"Runtime vs number of reads processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76b2337-3172-4383-a63f-eedfea506871",
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = []\n",
    "for reads, secs in zip(list(total_seconds_for_reads.keys()), list(total_seconds_for_reads.values())):\n",
    "    rate = reads/secs\n",
    "    rates.append(rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22692d03-d308-44b2-8dd3-ed39c20b6166",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(rates)), rates)\n",
    "plt.title(\"Mean rate (reads per second)\")\n",
    "plt.ylabel(\"Reads/Second\")\n",
    "plt.xlabel(\"Number of reads (e10^6)\")\n",
    "\n",
    "average_rate = np.mean(rates)\n",
    "plt.axhline(average_rate, color='r')\n",
    "print(\"Average of {} reads/second\".format(average_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44894b19-e631-46ac-ad91-f9dbac3d91fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds_per_read = 1/average_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d0c4d3-f985-4275-8a78-1a176188eae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "reads_per_cell = 50000\n",
    "total_cells = 10000\n",
    "total_reads = reads_per_cell * total_cells\n",
    "print(total_reads)\n",
    "\n",
    "total_estimated_time = total_reads * seconds_per_read\n",
    "print('Estimated total time in minutes for {} reads: {} minutes'.format(total_reads, math.ceil(total_estimated_time/60), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e49151-ba3c-40a4-9290-3d8360753d96",
   "metadata": {},
   "source": [
    "# Second loop to get coverage at sites with edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb4885-1a35-4ed6-845e-8b64c34ea558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: get all edit info files for each contig and group them by contig, before processing.\n",
    "# Todo: Multiprocess that too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab82bcb-c420-41d2-bd70-95f3958f54fc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "splits = [i.split(\"/\")[-1].split('_edit')[0] for i in glob('{}/edit_info/*'.format(output_folder))]\n",
    "print(\"Accessing split bams: {}\".format(', '.join(sorted(splits))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7b58ee-46a1-41e1-b3d5-d1aa4959975f",
   "metadata": {},
   "source": [
    "### Gather the edit information generated for each subcontig, and group by contig so we only have 1 edit information dataframe to process per contig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aa1ff3-69e7-4a5c-8cee-d6af8008eec0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_edit_info_for_barcodes = []\n",
    "\n",
    "edit_info_grouped_per_contig = defaultdict(lambda:[])\n",
    "edit_info_grouped_per_contig_combined = defaultdict(lambda:[])\n",
    "\n",
    "num_splits = len(splits)\n",
    "print(\"Grouping edit information outputs by contig...\")\n",
    "for i, split in enumerate(splits):\n",
    "    if i%10 == 0:\n",
    "        print(\"\\t{}/{}...\".format(i, num_splits))\n",
    "    contig = split.split(\"_\")[0]\n",
    "    \n",
    "    barcode_to_coverage_dict = defaultdict()    \n",
    "    \n",
    "    barcode_to_coverage_dict = defaultdict()\n",
    "    edit_info_file = '{}/edit_info/{}_edit_info.tsv'.format(output_folder, split)\n",
    "    edit_info = pd.read_csv(edit_info_file, sep='\\t')\n",
    "    edit_info_grouped_per_contig[contig].append(edit_info)\n",
    "print(\"Done grouping! Concatenating ...\")\n",
    "\n",
    "for contig, list_of_edit_info_dfs in edit_info_grouped_per_contig.items():\n",
    "    edit_info_grouped_per_contig_combined[contig] = pd.concat(list_of_edit_info_dfs)\n",
    "\n",
    "print(\"Done concatenating!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ef51b-eeba-4fb5-a716-3d6cd204c1b6",
   "metadata": {},
   "source": [
    "### Get coverage at edit positions for each contig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c22ae8-9b11-490a-8913-9fbe3fc7ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "def get_edit_info_for_barcode_in_contig(edit_info, contig, barcode, output_folder):\n",
    "    \n",
    "    bam_subfolder = \"{}/split_bams/{}\".format(output_folder, contig)\n",
    "    barcode_bam = '{}/{}_{}.bam'.format(bam_subfolder, contig, barcode)\n",
    "\n",
    "    samfile_for_barcode = pysam.AlignmentFile(barcode_bam, \"rb\")\n",
    "\n",
    "    edit_info_for_barcode = edit_info[edit_info.barcode == barcode]\n",
    "    positions_for_barcode = edit_info_for_barcode.position.tolist()\n",
    "\n",
    "    coverage = []\n",
    "    for pos in positions_for_barcode:\n",
    "        coverage_at_pos = np.sum(samfile_for_barcode.count_coverage(contig, pos-1, pos, quality_threshold=0))\n",
    "        coverage.append(coverage_at_pos)\n",
    "\n",
    "    edit_info_for_barcode['coverage'] = coverage\n",
    "    edit_info_for_barcode['contig'] = edit_info_for_barcode.contig.astype(str)\n",
    "\n",
    "    return edit_info_for_barcode\n",
    "\n",
    "\n",
    "def get_edit_info_for_barcode_in_contig_wrapper(parameters):\n",
    "    edit_info, contig, barcode, output_folder = parameters\n",
    "    edit_info_for_barcode = get_edit_info_for_barcode_in_contig(edit_info, contig, barcode, output_folder)\n",
    "    return edit_info_for_barcode\n",
    "\n",
    "\n",
    "def get_coverage_for_edits_in_contig(edit_info_grouped_per_contig_combined, output_folder):\n",
    "    job_params = []\n",
    "    \n",
    "    for contig, edit_info in edit_info_grouped_per_contig_combined.items():\n",
    "        unique_barcodes = sorted(edit_info.barcode.unique())\n",
    "\n",
    "        for i, barcode in enumerate(unique_barcodes):                 \n",
    "            job_params.append([edit_info, contig, barcode, output_folder])  \n",
    "    return job_params\n",
    "    \n",
    "coverage_counting_job_params = get_coverage_for_edits_in_contig(edit_info_grouped_per_contig_combined, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f90222-7506-42ea-abac-c6c55966785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "getsizeof(coverage_counting_job_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851efc7-2766-4666-b426-a1ab243db920",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "results = []\n",
    "with Pool(processes=16) as p:\n",
    "    max_ = len(coverage_counting_job_params)\n",
    "    with tqdm(total=max_) as pbar:\n",
    "        for _ in p.imap_unordered(get_edit_info_for_barcode_in_contig_wrapper, coverage_counting_job_params):\n",
    "            pbar.update()\n",
    "            results.append(_)\n",
    "            \n",
    "total_time = time.perf_counter() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd648239-8132-4ed1-b324-09e03b198a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7caa846-b0c5-4783-b7b7-984d8f8658ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edit_info = pd.concat(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8620db3-658d-4e62-a352-302ce84c4c8c",
   "metadata": {},
   "source": [
    "# Group by site to get final total edit and coverage counts at each site"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03097c3-ad74-474b-8af6-921e9fc031e0",
   "metadata": {},
   "source": [
    "# Verify C>T ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803f5b24-4714-4309-a08b-9b9445218c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edit_info.groupby(['ref', 'alt']).count().plot(kind='barh', legend=False)\n",
    "plt.title(\"All edits\")\n",
    "\n",
    "base_quality_thresh = 15\n",
    "all_edit_info[all_edit_info.base_quality > base_quality_thresh].groupby(['ref', 'alt']).count().plot(kind='barh', legend=False)\n",
    "plt.title(\"Edits with base quality > {}\".format(base_quality_thresh))\n",
    "\n",
    "all_edit_info_filtered = all_edit_info[all_edit_info.base_quality > base_quality_thresh]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a97fbec-9329-4c9c-8c9b-a785a8ae376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_new_ct =  all_edit_info_filtered[(all_edit_info_filtered.ref == 'C') & (all_edit_info_filtered.alt == 'T')].sort_values('position')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b3fdc3-7fab-448f-a9e9-52cbab695e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(example_new_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be865e2-f294-4152-ae0b-85d24509a1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_new_ct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc30c7f-73aa-4be6-83a7-ff2a2072d602",
   "metadata": {},
   "source": [
    "# Cells that do have STAMP expressed versus don't...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7ee3ff-374f-4a6e-8ab7-0b1e1311c07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stamp_expression_path = \\\n",
    "'/projects/ps-yeolab3/ekofman/Sammi/MouseBrainEF1A_SingleCell_EPR_combined/\\\n",
    "4.1_cells_with_middling_stamp/stamp_expression_for_all_used_cells.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2684ed2-1146-4e4c-9b4d-186865d6e4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "stamp_expression_df = pd.read_csv(stamp_expression_path, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093153f4-7d3a-4a04-bf6d-ee855e23a02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stamp_expression_df.Stamp.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26edccde-cad7-48fe-ab64-20d91bcf8844",
   "metadata": {},
   "outputs": [],
   "source": [
    "for thresh in [0, 1, 2, 3, 4, 5, 6, 6.5]:\n",
    "    print(thresh)\n",
    "    barcodes_at_stamp_thresh = stamp_expression_df[stamp_expression_df.Stamp > thresh].index\n",
    "    \n",
    "    all_edit_info_filtered[\n",
    "        all_edit_info_filtered.barcode.isin(barcodes_at_stamp_thresh)].groupby(['ref', 'alt']).count().plot(kind='barh', legend=False)\n",
    "    plt.title(\"Edit Type Distribution for Cells with STAMP expression above {}\".format(thresh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8800507-1d95-4bb6-94a8-9c0ec7de29fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for thresh in [1.5,2, 3, 4, 5, 6]:\n",
    "    print(thresh)\n",
    "    barcodes_at_stamp_thresh = stamp_expression_df[stamp_expression_df.Stamp < thresh].index\n",
    "    \n",
    "    all_edit_info_filtered[\n",
    "        all_edit_info_filtered.barcode.isin(barcodes_at_stamp_thresh)].groupby(['ref', 'alt']).count().plot(kind='barh', legend=False)\n",
    "    plt.title(\"Edit Type Distribution for Cells with STAMP expression below {}\".format(thresh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc326f-a695-41f6-bafb-5daf952a4dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71e31ad-c7fb-4c3d-b1f2-84ed484cb525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_anaconda3",
   "language": "python",
   "name": "new_anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
